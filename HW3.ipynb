{"metadata":{"colab":{"name":"homework3_student.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Homework 3: train RNN-T","metadata":{"id":"1LxiMiEui-kl"}},{"cell_type":"markdown","source":"In this homework you will implement a variant of the RNN-T model. For that, you will have to\n- implement each part of its architecture: Encoder, Predictor, Joiner\n- implement the greedy decoding algorithm\n- train your model on a subset of the LibriSpeech corpus","metadata":{"id":"NfPduSa6i-km"}},{"cell_type":"markdown","source":"# Setup - Install package, download files, etc...","metadata":{"id":"AboXphY7i-kp"}},{"cell_type":"code","source":"!mkdir files\n!wget -O files/utils.py https://raw.githubusercontent.com/severilov/DL-Audio-AIMasters-Course/main/seminars/seminar04/files/utils.py","metadata":{"id":"6tO44Q_hi-kp","scrolled":true,"execution":{"iopub.status.busy":"2023-04-26T00:41:52.640789Z","iopub.execute_input":"2023-04-26T00:41:52.641902Z","iopub.status.idle":"2023-04-26T00:41:55.026461Z","shell.execute_reply.started":"2023-04-26T00:41:52.641862Z","shell.execute_reply":"2023-04-26T00:41:55.025081Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"--2023-04-26 00:41:54--  https://raw.githubusercontent.com/severilov/DL-Audio-AIMasters-Course/main/seminars/seminar04/files/utils.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 6770 (6.6K) [text/plain]\nSaving to: ‘files/utils.py’\n\nfiles/utils.py      100%[===================>]   6.61K  --.-KB/s    in 0s      \n\n2023-04-26 00:41:54 (56.4 MB/s) - ‘files/utils.py’ saved [6770/6770]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"!wget --load-cookies /tmp/cookies.txt \"https://drive.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://drive.google.com/uc?export=download&id=14vgOVBayQGYv9B1P3hYo3JM56rS6ap3U' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=14vgOVBayQGYv9B1P3hYo3JM56rS6ap3U\" -O files/model_scripted_epoch_5.pt && rm -rf /tmp/cookies.txt","metadata":{"id":"gmrbnsCEi-kq","scrolled":true,"execution":{"iopub.status.busy":"2023-04-26T00:41:55.029327Z","iopub.execute_input":"2023-04-26T00:41:55.029973Z","iopub.status.idle":"2023-04-26T00:42:01.770869Z","shell.execute_reply.started":"2023-04-26T00:41:55.029929Z","shell.execute_reply":"2023-04-26T00:42:01.769612Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"--2023-04-26 00:41:56--  https://drive.google.com/uc?export=download&confirm=t&id=14vgOVBayQGYv9B1P3hYo3JM56rS6ap3U\nResolving drive.google.com (drive.google.com)... 209.85.147.139, 209.85.147.138, 209.85.147.100, ...\nConnecting to drive.google.com (drive.google.com)|209.85.147.139|:443... connected.\nHTTP request sent, awaiting response... 303 See Other\nLocation: https://doc-0c-9s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/gpkphg8uop39d6jto1ag6qp8h3c6qplb/1682469675000/02999746975866030610/*/14vgOVBayQGYv9B1P3hYo3JM56rS6ap3U?e=download&uuid=72f8735b-1b89-48e2-b55d-e02a96b5f4a3 [following]\nWarning: wildcards not supported in HTTP.\n--2023-04-26 00:41:56--  https://doc-0c-9s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/gpkphg8uop39d6jto1ag6qp8h3c6qplb/1682469675000/02999746975866030610/*/14vgOVBayQGYv9B1P3hYo3JM56rS6ap3U?e=download&uuid=72f8735b-1b89-48e2-b55d-e02a96b5f4a3\nResolving doc-0c-9s-docs.googleusercontent.com (doc-0c-9s-docs.googleusercontent.com)... 74.125.124.132, 2607:f8b0:4001:c14::84\nConnecting to doc-0c-9s-docs.googleusercontent.com (doc-0c-9s-docs.googleusercontent.com)|74.125.124.132|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 44610317 (43M) [application/x-zip]\nSaving to: ‘files/model_scripted_epoch_5.pt’\n\nfiles/model_scripte 100%[===================>]  42.54M  9.51MB/s    in 4.5s    \n\n2023-04-26 00:42:01 (9.51 MB/s) - ‘files/model_scripted_epoch_5.pt’ saved [44610317/44610317]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"%%capture\n!pip install wandb -qqq","metadata":{"id":"hcfHw_yWi-kr","execution":{"iopub.status.busy":"2023-04-26T00:42:01.773720Z","iopub.execute_input":"2023-04-26T00:42:01.774145Z","iopub.status.idle":"2023-04-26T00:42:15.080174Z","shell.execute_reply.started":"2023-04-26T00:42:01.774102Z","shell.execute_reply":"2023-04-26T00:42:15.078638Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"!nvcc --version","metadata":{"execution":{"iopub.status.busy":"2023-04-26T00:42:15.082201Z","iopub.execute_input":"2023-04-26T00:42:15.082556Z","iopub.status.idle":"2023-04-26T00:42:16.073606Z","shell.execute_reply.started":"2023-04-26T00:42:15.082523Z","shell.execute_reply":"2023-04-26T00:42:16.072463Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"nvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2021 NVIDIA Corporation\nBuilt on Mon_May__3_19:15:13_PDT_2021\nCuda compilation tools, release 11.3, V11.3.109\nBuild cuda_11.3.r11.3/compiler.29920130_0\n","output_type":"stream"}]},{"cell_type":"code","source":"conda install pytorch==1.12.1 torchvision==0.13.1 torchaudio==0.12.1 cudatoolkit=11.3 -c pytorch","metadata":{"execution":{"iopub.status.busy":"2023-04-26T00:42:16.077197Z","iopub.execute_input":"2023-04-26T00:42:16.077538Z","iopub.status.idle":"2023-04-26T00:44:08.157619Z","shell.execute_reply.started":"2023-04-26T00:42:16.077497Z","shell.execute_reply":"2023-04-26T00:44:08.156141Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Collecting package metadata (current_repodata.json): done\nSolving environment: failed with initial frozen solve. Retrying with flexible solve.\nCollecting package metadata (repodata.json): - ^C\n| \nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport string\nfrom typing import Tuple, List, Dict, Optional\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torch\nimport torchaudio\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.data as data\nimport wandb\nimport ipywidgets as widgets\nimport itertools\nfrom torch import optim\nfrom torchaudio.transforms import RNNTLoss\nfrom tqdm import tqdm_notebook, tqdm, notebook\nfrom IPython.display import display, clear_output","metadata":{"id":"4u_exS1ii-ks","execution":{"iopub.status.busy":"2023-04-26T00:37:49.393890Z","iopub.execute_input":"2023-04-26T00:37:49.394615Z","iopub.status.idle":"2023-04-26T00:37:55.628956Z","shell.execute_reply.started":"2023-04-26T00:37:49.394574Z","shell.execute_reply":"2023-04-26T00:37:55.627441Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import files.utils as utils ","metadata":{"id":"VceAkujPi-ks","execution":{"iopub.status.busy":"2023-04-26T00:44:15.776109Z","iopub.execute_input":"2023-04-26T00:44:15.777412Z","iopub.status.idle":"2023-04-26T00:44:15.787264Z","shell.execute_reply.started":"2023-04-26T00:44:15.777366Z","shell.execute_reply":"2023-04-26T00:44:15.786069Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"snapshot_dir = \"/mnt/notebooks/rnn_t_snapshots\"","metadata":{"id":"n14D_Nzei-ks","execution":{"iopub.status.busy":"2023-04-26T00:44:16.159634Z","iopub.execute_input":"2023-04-26T00:44:16.160522Z","iopub.status.idle":"2023-04-26T00:44:16.170732Z","shell.execute_reply.started":"2023-04-26T00:44:16.160476Z","shell.execute_reply":"2023-04-26T00:44:16.169462Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"!mkdir /mnt/notebooks/rnn_t_snapshots","metadata":{"id":"txn-Rm7Si-kt","outputId":"0bcb10f2-7160-4b54-da4d-ad1f70022502","execution":{"iopub.status.busy":"2023-04-26T00:44:16.425358Z","iopub.execute_input":"2023-04-26T00:44:16.425930Z","iopub.status.idle":"2023-04-26T00:44:17.615968Z","shell.execute_reply.started":"2023-04-26T00:44:16.425886Z","shell.execute_reply":"2023-04-26T00:44:17.614600Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"mkdir: cannot create directory ‘/mnt/notebooks/rnn_t_snapshots’: No such file or directory\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Seminar 4 RECAP: RNN-T Forward-Backward Algorithm","metadata":{"id":"o8kaGJYEi-kn"}},{"cell_type":"markdown","source":"In seminar 4 we have implememented forward and backward algorithms for calculating the RNN-T loss.","metadata":{"id":"Lq4a_Bwe8vqi"}},{"cell_type":"code","source":"def forward(log_probs: torch.FloatTensor, targets: torch.LongTensor, \n            blank: int = -1) -> Tuple[torch.FloatTensor, torch.FloatTensor]:\n    \"\"\"\n    :param log_probs: model outputs after applying log_softmax\n    :param targets: the target sequence of tokens, represented as integer indexes\n    :param blank: the index of blank symbol\n    :return: Tuple[ln alpha, -(ln alpha(T, U) + ln P(blank | T, U))]. \n        The latter term is loss value, which is -ln P(y | x)\n    \"\"\"\n    max_T, max_U, D = log_probs.shape\n    \n    # here the alpha variable contains logarithm of the alpha variable from the formulas above\n    alpha = np.zeros((max_T, max_U), dtype=np.float32)\n\n    for t in range(1, max_T):\n        alpha[t, 0] = alpha[t-1, 0] + log_probs[t-1, 0, blank]\n\n    for u in range(1, max_U):\n        alpha[0, u] = alpha[0, u-1] + log_probs[0, u-1, targets[min(u-1, len(targets)-1)]]\n\n    for t in range(1, max_T):\n        for u in range(1, max_U):\n            alpha[t, u] =  np.logaddexp(\n                alpha[t-1, u] + log_probs[t-1, u, blank],\n                alpha[t, u-1] + log_probs[t, u-1, targets[min(u-1, len(targets)-1)]]\n            )\n\n    cost = - (log_probs[-1, -1, blank] + alpha[-1, -1]) \n    return alpha, cost\n\n\ndef backward(log_probs: torch.FloatTensor, targets: torch.LongTensor, \n             blank: int = -1) -> Tuple[torch.FloatTensor, torch.FloatTensor]:\n    \"\"\"\n    :param log_probs: model outputs after applying log_softmax\n    :param targets: the target sequence of tokens, represented as integer indexes\n    :param blank: the index of blank symbol\n    :return: Tuple[ln beta, -ln beta(0, 0)]. The latter term is loss value, which is -ln P(y | x)\n    \"\"\"\n    max_T, max_U, D = log_probs.shape\n    \n    # here the beta variable contains logarithm of the beta variable from the formulas above\n    beta = np.zeros((max_T, max_U), dtype=np.float32)\n    beta[-1, -1] = log_probs[-1, -1, blank]\n\n    for t in reversed(range(max_T - 1)):\n        beta[t, max_U-1] = beta[t+1, max_U-1] + log_probs[t, max_U-1, blank]\n\n    for u in reversed(range(max_U - 1)):\n        beta[max_T-1, u] = beta[max_T-1, u+1] + log_probs[max_T-1, u, targets[min(u, len(targets)-1)]]\n\n    for t in reversed(range(max_T - 1)):\n        for u in reversed(range(max_U - 1)):\n            beta[t, u] =  np.logaddexp(\n                beta[t+1, u] + log_probs[t, u, blank],\n                beta[t, u+1] + log_probs[t, u, targets[min(u, len(targets)-1)]]\n            )\n            \n    cost = - beta[0, 0]\n    return beta, cost","metadata":{"id":"EcM082LQi-kz","execution":{"iopub.status.busy":"2023-04-26T00:44:17.619960Z","iopub.execute_input":"2023-04-26T00:44:17.620930Z","iopub.status.idle":"2023-04-26T00:44:17.656246Z","shell.execute_reply.started":"2023-04-26T00:44:17.620876Z","shell.execute_reply":"2023-04-26T00:44:17.652435Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Implementing, training and evaluating your RNN-T ASR model (30 points)","metadata":{"id":"e-Kcnm7Ki-k1"}},{"cell_type":"markdown","source":"```\n[ ] (10 points) Build the model\n[ ] (10 points) Implementing a greedy decoder\n[ ] (10 points) Train the model \n```","metadata":{"id":"u7_XForhi-k1"}},{"cell_type":"code","source":"BLANK_SYMBOL = \"_\"\nBOS = \"<BOS>\"\n\n\nclass Tokenizer:\n    \"\"\"\n    Maps characters to integers and vice versa\n    \"\"\"\n    def __init__(self):\n        self.char_map = {}\n        self.index_map = {}\n        for i, ch in enumerate([\"'\", \" \"] + list(string.ascii_lowercase) + [BLANK_SYMBOL, BOS]):\n            self.char_map[ch] = i\n            self.index_map[i] = ch\n        \n    def text_to_indices(self, text: str) -> List[int]:\n        \"\"\"\n        Maps string to a list of integers\n        \"\"\"\n        return [self.char_map[ch] for ch in text]\n\n    def indices_to_text(self, labels: List[int]) -> str:\n        \"\"\"\n        Maps integers back to text\n        \"\"\"\n        return \"\".join([self.index_map[i] for i in labels])\n    \n    def get_symbol_index(self, sym: str) -> int:\n        \"\"\"\n        Returns index for the specified symbol\n        \"\"\"\n        return self.char_map[sym]\n    \n\ntokenizer = Tokenizer()","metadata":{"id":"TIy1M2ICi-k1","execution":{"iopub.status.busy":"2023-04-26T00:44:18.104729Z","iopub.execute_input":"2023-04-26T00:44:18.105131Z","iopub.status.idle":"2023-04-26T00:44:18.113704Z","shell.execute_reply.started":"2023-04-26T00:44:18.105095Z","shell.execute_reply":"2023-04-26T00:44:18.112589Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Utils for creating a dataloader","metadata":{"id":"t_QF8XMBi-k2"}},{"cell_type":"code","source":"# Download LibriSpeech 100hr training and test data\n\nif not os.path.isdir(\"./data\"):\n    os.makedirs(\"./data\")\n\ntrain_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=\"train-clean-100\", download=True)\ntest_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=\"test-clean\", download=True)","metadata":{"id":"L_Xmk902i-k2","execution":{"iopub.status.busy":"2023-04-26T00:44:18.711718Z","iopub.execute_input":"2023-04-26T00:44:18.712239Z","iopub.status.idle":"2023-04-26T00:50:04.995450Z","shell.execute_reply.started":"2023-04-26T00:44:18.712205Z","shell.execute_reply":"2023-04-26T00:50:04.994319Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/5.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc4fa4810d4b44cc8e10986799ce6986"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/331M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1d459ee1db24ef5aa0b9addccba25fa"}},"metadata":{}}]},{"cell_type":"code","source":"# For train you can use SpecAugment data aug here.\ntrain_audio_transforms = nn.Sequential(\n    torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=80),\n    torchaudio.transforms.FrequencyMasking(freq_mask_param=27),\n    torchaudio.transforms.TimeMasking(time_mask_param=100)\n)\n\ntest_audio_transforms = torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=80)","metadata":{"id":"mLWxbcjMi-k2","execution":{"iopub.status.busy":"2023-04-26T00:50:40.406126Z","iopub.execute_input":"2023-04-26T00:50:40.407158Z","iopub.status.idle":"2023-04-26T00:50:40.416444Z","shell.execute_reply.started":"2023-04-26T00:50:40.407119Z","shell.execute_reply":"2023-04-26T00:50:40.415260Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def data_processing(data: torchaudio.datasets.librispeech.LIBRISPEECH, \n                    data_type: str = \"train\") -> Tuple[torch.Tensor, torch.IntTensor, torch.IntTensor, torch.IntTensor]:\n    \"\"\"\n    :param data: a LIBRISPEECH dataset\n    :param data_type: \"train\" or \"test\"\n    :return: tuple of\n        spectrograms, shape: (B, T, n_mels)\n        labels, shape: (B, U)\n        input_lengths -- the length of each spectrogram in the batch, shape: (B,)\n        label_lengths -- the length of each text label in the batch, shape: (B,)\n        where\n        B: batch size\n        T: maximum source sequence length in batch\n        U: maximum target sequence length in batch\n        D: feature dimension of each source sequence element\n    \"\"\"\n    spectrograms = []\n    labels = []\n    input_lengths = []\n    label_lengths = []\n    for (waveform, _, utterance, _, _, _) in data:\n        if data_type == 'train':\n            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n        elif data_type == 'test':\n            spec = test_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n        else:\n            raise Exception('data_type should be train or valid')\n        spectrograms.append(spec)\n        label = torch.IntTensor(tokenizer.text_to_indices(utterance.lower()))\n        labels.append(label)\n        input_lengths.append(spec.shape[0])\n        label_lengths.append(len(label))\n\n    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True)\n    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n\n    return spectrograms, torch.IntTensor(labels), torch.IntTensor(input_lengths), torch.IntTensor(label_lengths)\n","metadata":{"id":"cLsCL472i-k2","execution":{"iopub.status.busy":"2023-04-26T00:50:40.669548Z","iopub.execute_input":"2023-04-26T00:50:40.669946Z","iopub.status.idle":"2023-04-26T00:50:40.679747Z","shell.execute_reply.started":"2023-04-26T00:50:40.669913Z","shell.execute_reply":"2023-04-26T00:50:40.678543Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"## Build the model (10 points)","metadata":{"id":"1SbrcCk6i-k3"}},{"cell_type":"code","source":"class EncoderRNNT(nn.Module):\n    def __init__(self, input_dim: int, hidden_size: int, output_dim: int, n_layers: int, \n                 dropout: float = 0.2, bidirectional: bool = True):\n        \"\"\"\n        An RNN-based model that encodes input audio features into a hidden representation. \n        The architecture is a stack of LSTM's followed by a fully-connected output layer.\n        \n        :param input_dim: the number of mel-spectrogram features\n        :param hidden_size: the number of features in the hidden states in LSTM layers\n        :param output_dim: the output dimension\n        :param n_layers: the number of stacked LSTM layers\n        :param dropout: the dropout probability for LSTM layers\n        :param bidirectional: If True, each LSTM layer becomes bidirectional\n        \"\"\"\n        super().__init__()\n\n        self.lstm = nn.LSTM(input_size = input_dim, \n                            hidden_size = hidden_size, \n                            num_layers = n_layers, \n                            dropout = dropout,\n                            bidirectional = bidirectional,\n                            batch_first=True)\n        \n        linear_in = 1 + int(bidirectional)\n        self.output_proj = nn.Linear(linear_in * hidden_size, output_dim)\n\n    def forward(self, inputs: torch.Tensor, input_lengths: torch.Tensor) -> Tuple[torch.Tensor, List[torch.Tensor]]:\n        \"\"\"        \n        :param inputs: spectrograms, shape: (B, T, n_mels)\n        :param input_lengths: the lengths of the spectrograms in the batch, shape: (B,)\n        :return: outputs of the projection layer and hidden states from LSTMs\n        \"\"\"\n        x, hidden = self.lstm(inputs)\n        logits = self.output_proj(x)\n        return logits, hidden","metadata":{"id":"y57EYzcgi-k3","execution":{"iopub.status.busy":"2023-04-26T00:50:41.236219Z","iopub.execute_input":"2023-04-26T00:50:41.237082Z","iopub.status.idle":"2023-04-26T00:50:41.250698Z","shell.execute_reply.started":"2023-04-26T00:50:41.237041Z","shell.execute_reply":"2023-04-26T00:50:41.246405Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"encoder = EncoderRNNT(\n    input_dim=80,\n    hidden_size=320,\n    output_dim=512, \n    n_layers=4,\n    dropout=0.2,\n    bidirectional=True\n)\n\nloader = data.DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn=lambda x: data_processing(x, 'test'))\nspectrograms, labels, input_lengths, label_lengths = next(iter(loader))\nlogits, hidden_states = encoder.forward(spectrograms, input_lengths)\n\nassert spectrograms.shape == torch.Size([2, 835, 80])\nassert logits.shape == torch.Size([2, 835, 512])\nassert len(hidden_states) == 2\nassert hidden_states[0].shape == torch.Size([8, 2, 320])","metadata":{"id":"tCeaQG-2i-k4","execution":{"iopub.status.busy":"2023-04-26T00:50:41.500943Z","iopub.execute_input":"2023-04-26T00:50:41.501662Z","iopub.status.idle":"2023-04-26T00:50:43.244125Z","shell.execute_reply.started":"2023-04-26T00:50:41.501625Z","shell.execute_reply":"2023-04-26T00:50:43.242982Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"class DecoderRNNT(nn.Module):\n    def __init__(self, hidden_size: int, vocab_size: int, output_dim: int, n_layers: int, dropout: float = 0.2):\n        \"\"\"\n        A simple RNN-based autoregressive language model that takes as input previously generated text tokens\n        and outputs a hidden representation of the next token\n\n        :param hidden_size: the number of features in the hidden states in LSTM layers\n        :param vocab_size: the number of text tokens in the dictionary\n        :param output_dim: the output dimension\n        :param n_layers: the number of stacked LSTM layers\n        :param dropout: the dropout probability for LSTM layers\n        \"\"\"\n        super().__init__()\n        self.embedding = nn.Embedding(num_embeddings = vocab_size,\n                                      embedding_dim = hidden_size)\n        \n        self.lstm = nn.LSTM(input_size = hidden_size, \n                            hidden_size = hidden_size, \n                            num_layers = n_layers, \n                            dropout = dropout,\n                            batch_first=True)\n        \n        self.output_proj = nn.Linear(hidden_size, output_dim)\n\n    def forward(self, inputs: torch.Tensor, input_lengths: Optional[torch.Tensor] = None, \n                hidden_states: Optional[Tuple[torch.Tensor, torch.Tensor]] = None) -> Tuple[torch.Tensor, List[torch.Tensor]]:\n        \"\"\"        \n        :param inputs: labels, shape: (B, U)\n        :param input_lengths: the lengths of the text labels in the batch, shape: (B,)\n        :return: outputs of the projection layer and hidden states from LSTMs\n        \"\"\"\n        embed_inputs = self.embedding(inputs)\n\n        if input_lengths is not None:\n            # training phase, the code here is close to `forward` of the Encoder \n            x, hidden = self.lstm(embed_inputs)\n            outputs = self.output_proj(x)\n        else:\n            # testing phase\n            outputs, hidden = self.lstm(embed_inputs, hidden_states)\n\n        outputs = self.output_proj(outputs)\n        return outputs, hidden","metadata":{"id":"bhLTyplNi-k4","execution":{"iopub.status.busy":"2023-04-26T00:50:43.246535Z","iopub.execute_input":"2023-04-26T00:50:43.246973Z","iopub.status.idle":"2023-04-26T00:50:43.258159Z","shell.execute_reply.started":"2023-04-26T00:50:43.246929Z","shell.execute_reply":"2023-04-26T00:50:43.256892Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"decoder = DecoderRNNT(\n    hidden_size=512,\n    vocab_size=len(tokenizer.char_map),\n    output_dim=512, \n    n_layers=1, \n    dropout=0.2\n)\n\nloader = data.DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn=lambda x: data_processing(x, 'test'))\nspectrograms, labels, input_lengths, label_lengths = next(iter(loader))\nlogits, hidden_states = decoder.forward(labels, label_lengths)\n\nassert labels.shape == torch.Size([2, 158])\nassert logits.shape == torch.Size([2, 158, 512])\nassert len(hidden_states) == 2\nassert hidden_states[0].shape == torch.Size([1, 2, 512])","metadata":{"id":"QNGRqpMji-k4","execution":{"iopub.status.busy":"2023-04-26T00:50:43.260207Z","iopub.execute_input":"2023-04-26T00:50:43.261180Z","iopub.status.idle":"2023-04-26T00:50:43.445551Z","shell.execute_reply.started":"2023-04-26T00:50:43.261138Z","shell.execute_reply":"2023-04-26T00:50:43.444402Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"class Joiner(torch.nn.Module):\n    def __init__(self, joiner_dim: int, num_outputs: int):\n        \"\"\"\n        Adds encoder and decoder outputs, applies ReLU and passes the result \n        through a fully connected layer to get the output logits\n        \n        :param joiner_dim: the dimension of the encoder and decoder outputs\n        :num_outputs: the number of text tokens in the dictionary\n        \"\"\"\n        super().__init__()\n        self.linear = nn.Linear(joiner_dim, num_outputs)\n        self.relu = nn.ReLU()\n        self.softmax = nn.Softmax(dim=-1)\n        \n    def forward(self, encoder_outputs: torch.Tensor, decoder_outputs: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        :param encoder_outputs: the encoder outputs (f_t), shape: (B, T, joiner_dim) or (joiner_dim,)\n        :param decoder_outputs: the decoder outputs (g_u), shape: (B, U, joiner_dim) or (joiner_dim,)\n        :return: output logits\n        \"\"\"\n        if encoder_outputs.dim() == 3 and decoder_outputs.dim() == 3:    # True for training phase\n            encoder_outputs = encoder_outputs.unsqueeze(2)\n            decoder_outputs = decoder_outputs.unsqueeze(1)\n        \n        # Linear(ReLU(f_t + g_u))\n        out = self.linear(self.relu(encoder_outputs + decoder_outputs))\n        # out = self.softmax(out)\n        return out","metadata":{"id":"IaM_Tkzhi-k5","execution":{"iopub.status.busy":"2023-04-26T00:50:43.447657Z","iopub.execute_input":"2023-04-26T00:50:43.447943Z","iopub.status.idle":"2023-04-26T00:50:43.459417Z","shell.execute_reply.started":"2023-04-26T00:50:43.447916Z","shell.execute_reply":"2023-04-26T00:50:43.457103Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"class RNNTransducer(torch.nn.Module):\n    def __init__(self,\n        num_classes: int,\n        input_dim: int,\n        num_encoder_layers: int = 4,\n        num_decoder_layers: int = 1,\n        encoder_hidden_state_dim: int = 320,\n        decoder_hidden_state_dim: int = 512,\n        output_dim: int = 512,\n        encoder_is_bidirectional: bool = True,\n        encoder_dropout_p: float = 0.2,\n        decoder_dropout_p: float = 0.2\n    ):\n        \"\"\"\n        :param num_classes: the number of text tokens in the dictionary\n        :param input_dim: the number of mel-spectrogram features\n        :param num_encoder_layers: the number of LSTM layers in the encoder\n        :param num_decoder_layers: the number of LSTM layers in the decoder\n        :param encoder_hidden_state_dim: the number of features in the hidden states for the encoder\n        :param decoder_hidden_state_dim: the number of features in the hidden states for the decoder\n        :param output_dim: the output dimension\n        :param encoder_is_bidirectional: whether to use bidirectional LSTM's in the encoder\n        :param encoder_dropout_p: the dropout probability for the encoder\n        :param decoder_dropout_p: the dropout probability for the decoder\n        \"\"\"\n        super().__init__()\n        self.encoder =  EncoderRNNT(input_dim=input_dim,\n                                    hidden_size=encoder_hidden_state_dim,\n                                    output_dim=output_dim, \n                                    n_layers=num_encoder_layers,\n                                    dropout=encoder_dropout_p,\n                                    bidirectional=encoder_is_bidirectional)\n        \n        # The decoder takes the input <BOS> + the original sequence. \n        # You need to shift the current label, and F.pad can help with that.\n        self.decoder =  DecoderRNNT(hidden_size=decoder_hidden_state_dim,\n                                    vocab_size=num_classes,\n                                    output_dim=output_dim, \n                                    n_layers=num_decoder_layers, \n                                    dropout=decoder_dropout_p)\n        self.joiner = Joiner(joiner_dim=output_dim,num_outputs=len(tokenizer.char_map))\n\n    def forward(self, inputs: torch.Tensor, input_lengths: torch.Tensor, \n                targets: torch.Tensor, target_lengths: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        :param inputs: spectrograms, shape: (B, T, n_mels)\n        :param input_lengths: the lengths of the spectrograms in the batch, shape: (B,)\n        :param targets: labels, shape: (B, U)\n        :param target_lengths: the lengths of the text labels in the batch, shape: (B,)\n        :return: the output logits, shape: (B, T, U, n_tokens)\n        \"\"\"\n        encoder_outputs, _ = self.encoder(inputs, input_lengths)\n        decoder_outputs, _ = self.decoder(F.pad(targets,(1, 0), value=29), target_lengths)\n        joiner_out = self.joiner(encoder_outputs, decoder_outputs)\n        return joiner_out\n","metadata":{"id":"ovd_OawAi-k5","execution":{"iopub.status.busy":"2023-04-26T00:50:43.462298Z","iopub.execute_input":"2023-04-26T00:50:43.462815Z","iopub.status.idle":"2023-04-26T00:50:43.477141Z","shell.execute_reply.started":"2023-04-26T00:50:43.462773Z","shell.execute_reply":"2023-04-26T00:50:43.476067Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"transducer = RNNTransducer(\n    num_classes=len(tokenizer.char_map),\n    input_dim=80,\n    num_encoder_layers=4,\n    num_decoder_layers=1,\n    encoder_hidden_state_dim=320,\n    decoder_hidden_state_dim=512,\n    output_dim=512,\n    encoder_is_bidirectional=True,\n    encoder_dropout_p=0.2,\n    decoder_dropout_p=0.2\n)\n\nloader = data.DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn=lambda x: data_processing(x, 'test'))\nspectrograms, labels, input_lengths, label_lengths = next(iter(loader))\nresult = transducer.forward(spectrograms, input_lengths, labels, label_lengths)\nprint(label_lengths, input_lengths)\nassert spectrograms.shape == torch.Size([2, 835, 80])\nassert labels.shape == torch.Size([2, 158])\nprint(result.shape)\nassert result.shape == torch.Size([2, 835, 159, 30])","metadata":{"id":"PnC5EE33i-k6","execution":{"iopub.status.busy":"2023-04-26T00:50:47.326657Z","iopub.execute_input":"2023-04-26T00:50:47.327092Z","iopub.status.idle":"2023-04-26T00:50:49.862933Z","shell.execute_reply.started":"2023-04-26T00:50:47.327043Z","shell.execute_reply":"2023-04-26T00:50:49.861706Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"tensor([158,  42], dtype=torch.int32) tensor([835, 263], dtype=torch.int32)\ntorch.Size([2, 835, 159, 30])\n","output_type":"stream"}]},{"cell_type":"code","source":"for i in range(len(tokenizer.char_map)):\n    print(i, tokenizer.indices_to_text([i]))","metadata":{"execution":{"iopub.status.busy":"2023-04-26T00:50:49.864788Z","iopub.execute_input":"2023-04-26T00:50:49.865868Z","iopub.status.idle":"2023-04-26T00:50:49.873202Z","shell.execute_reply.started":"2023-04-26T00:50:49.865824Z","shell.execute_reply":"2023-04-26T00:50:49.872021Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"0 '\n1  \n2 a\n3 b\n4 c\n5 d\n6 e\n7 f\n8 g\n9 h\n10 i\n11 j\n12 k\n13 l\n14 m\n15 n\n16 o\n17 p\n18 q\n19 r\n20 s\n21 t\n22 u\n23 v\n24 w\n25 x\n26 y\n27 z\n28 _\n29 <BOS>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Implementing a greedy decoder (10 points)","metadata":{"id":"qBdlLxKni-k6"}},{"cell_type":"markdown","source":"<p style=\"text-align:center;\"><img src=\"http://drive.google.com/uc?export=view&id=1tHsoq0ZH0tHSHYlYlw00y8ksF-wHmrmC\">","metadata":{"id":"RPZv4-Bdi-k6"}},{"cell_type":"markdown","source":"Now we know how to train a Transducer, but how do we infer it? Our task is to generate an output sequence $\\mathbf y$ given an input acoustic sequence $\\mathbf x$.\n\nHere we will index the encoder outputs $f_t$ starting from zero, because it is more convenient when describing an algorithm.\n\nThe greedy decoding procedure is as follows:\n1. Compute $\\{f_0, \\ldots, f_T\\}$ using $\\mathbf x$.\n2. Set $t = 0$, $u = 0$, $\\mathbf y = []$, $\\mathrm{iteration} = 0$.\n3. If $u = 0$, set $g_0 = \\mathrm{Encoder}(\\langle s \\rangle)$. If $u > 0$, compute $g_u$ using the last predicted token $\\mathbf y[-1]$.\n4. Compute $P(y | t, u)$ using $f_t$ and $g_u$.\n5. If argmax of $P(y | t, u)$ is a label, set $u = u + 1$ and append the new label to $\\mathbf y$. \n6. If argmax of $P(y | t, u)$ is $\\emptyset$, set $t = t + 1$.\n7. If $t = T$ or $\\mathrm{iteration} = \\mathrm{max\\_iterations}$, we are done. Else, set $\\mathrm{iteration} = \\mathrm{iteration + 1}$ and go to step 3.","metadata":{"id":"wMhA58FEi-k6"}},{"cell_type":"code","source":"@torch.no_grad()\ndef greedy_decode(model: RNNTransducer, encoder_output: torch.Tensor, max_steps: int = 2000) -> torch.Tensor:\n    \"\"\"\n    :param model: an RNN-T model in eval mode\n    :param encoder_output: the output of the encoder part of RNN-T, shape: (T, encoder_output_dim)\n    :param max_steps: the maximum number of decoding steps\n    :return: the predicted labels\n    \"\"\"\n    pred_tokens, hidden_state = [], None\n    blank = tokenizer.get_symbol_index(BLANK_SYMBOL)    \n    max_time_steps = encoder_output.size(0)\n    t = 0\n    decoder_input = encoder_output.new_tensor([[tokenizer.get_symbol_index(BOS)]], dtype=torch.long)\n    decoder_output, hidden_state = model.decoder(decoder_input, hidden_states=hidden_state)\n\n    for _ in range(max_steps):\n        u = len(pred_tokens)\n        P = model.joiner(encoder_output[t], decoder_output).softmax(dim=-1)\n        token = int(P.argmax(dim=-1))\n        if token == blank:\n            t += 1\n        else:\n            pred_tokens.append(token)\n            decoder_input = encoder_output[t].new_tensor([[token]], dtype=torch.long)\n            decoder_output, hidden_state = model.decoder(decoder_input, hidden_states=hidden_state)\n        if t == max_time_steps:\n            break            \n\n    return torch.LongTensor(pred_tokens)\n\n\n@torch.no_grad()\ndef recognize(model: RNNTransducer, inputs: torch.Tensor, input_lengths: torch.Tensor) -> List[torch.Tensor]:\n    \"\"\"\n    :param model: an RNN-T model in eval mode\n    :param inputs: spectrograms, shape: (B, T, n_mels)\n    :param input_lengths: the lengths of the spectrograms in the batch, shape: (B,)\n    :return: a list with the predicted labels\n    \"\"\"\n    outputs = []\n    encoder_outputs, _ = model.encoder(inputs, input_lengths)\n\n    for encoder_output in encoder_outputs:\n        decoded_seq = greedy_decode(model, encoder_output)\n        outputs.append(decoded_seq)\n\n    return outputs\n\n\ndef get_transducer_predictions(\n        transducer: RNNTransducer, inputs: torch.Tensor, input_lengths: torch.Tensor, \n        targets: torch.Tensor, target_lengths: torch.Tensor\n    ) -> pd.DataFrame:\n    \"\"\"\n    :param transducer: an RNN-T model in eval mode\n    :param inputs: spectrograms, shape: (B, T, n_mels)\n    :param input_lengths: the lengths of the spectrograms in the batch, shape: (B,)\n    :param targets: labels, shape: (B, U)\n    :param target_lengths: the lengths of the text labels in the batch, shape: (B,)\n    :return: a pd.DataFrame with inference results\n    \"\"\"\n    predictions = recognize(transducer, inputs, input_lengths)\n    result = []\n    for pred, target, target_len in zip(predictions, targets, target_lengths):\n        label = target[:target_len]\n        utterance = tokenizer.indices_to_text(list(map(int, label)))\n        pred_utterance = tokenizer.indices_to_text(list(map(int, pred)))\n        result.append({\n            \"ground_truth\": utterance,\n            \"prediction\": pred_utterance,\n            \"cer\": utils.cer(utterance, pred_utterance),\n            \"wer\": utils.wer(utterance, pred_utterance)\n        })\n    return pd.DataFrame.from_records(result)\n","metadata":{"id":"o3tjfGsji-k7","execution":{"iopub.status.busy":"2023-04-26T00:50:49.911929Z","iopub.execute_input":"2023-04-26T00:50:49.912452Z","iopub.status.idle":"2023-04-26T00:50:49.927061Z","shell.execute_reply.started":"2023-04-26T00:50:49.912422Z","shell.execute_reply":"2023-04-26T00:50:49.925773Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"model = torch.jit.load('files/model_scripted_epoch_5.pt')\nmodel.eval()","metadata":{"id":"FIJtZpE3i-k7","execution":{"iopub.status.busy":"2023-04-26T00:50:50.133226Z","iopub.execute_input":"2023-04-26T00:50:50.133508Z","iopub.status.idle":"2023-04-26T00:50:50.492181Z","shell.execute_reply.started":"2023-04-26T00:50:50.133480Z","shell.execute_reply":"2023-04-26T00:50:50.491091Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"RecursiveScriptModule(\n  original_name=RNNTransducer\n  (encoder): RecursiveScriptModule(\n    original_name=EncoderRNNT\n    (lstm): RecursiveScriptModule(original_name=LSTM)\n    (output_proj): RecursiveScriptModule(original_name=Linear)\n  )\n  (decoder): RecursiveScriptModule(\n    original_name=DecoderRNNT\n    (embedding): RecursiveScriptModule(original_name=Embedding)\n    (lstm): RecursiveScriptModule(original_name=LSTM)\n    (output_proj): RecursiveScriptModule(original_name=Linear)\n  )\n  (joiner): RecursiveScriptModule(\n    original_name=Joiner\n    (linear): RecursiveScriptModule(original_name=Linear)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"loader = data.DataLoader(test_dataset, batch_size=5, shuffle=False, collate_fn=lambda x: data_processing(x, 'test'))\nspectrograms, labels, input_lengths, label_lengths = next(iter(loader))\npredictions = get_transducer_predictions(\n    model, spectrograms, input_lengths, \n    labels, label_lengths\n)\npredictions","metadata":{"id":"mu-sVO4Hi-k8","execution":{"iopub.status.busy":"2023-04-26T00:50:50.494461Z","iopub.execute_input":"2023-04-26T00:50:50.495547Z","iopub.status.idle":"2023-04-26T00:50:52.986377Z","shell.execute_reply.started":"2023-04-26T00:50:50.495487Z","shell.execute_reply":"2023-04-26T00:50:52.985092Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"                                        ground_truth  \\\n0  he hoped there would be stew for dinner turnip...   \n1         stuff it into you his belly counselled him   \n2  after early nightfall the yellow lamps would l...   \n3                 hello bertie any good in your mind   \n4  number ten fresh nelly is waiting on you good ...   \n\n                                          prediction       cer       wer  \n0  he hoped there would be stew for dinner turnip...  0.132911  0.250000  \n1           stuffed into you his belly counciled him  0.142857  0.375000  \n2  after early night fall the yellow lamps would ...  0.096154  0.333333  \n3              her about he and he good in your mind  0.352941  0.714286  \n4  none but den fresh now as waiting on you could...  0.254237  0.545455  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ground_truth</th>\n      <th>prediction</th>\n      <th>cer</th>\n      <th>wer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>he hoped there would be stew for dinner turnip...</td>\n      <td>he hoped there would be stew for dinner turnip...</td>\n      <td>0.132911</td>\n      <td>0.250000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>stuff it into you his belly counselled him</td>\n      <td>stuffed into you his belly counciled him</td>\n      <td>0.142857</td>\n      <td>0.375000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>after early nightfall the yellow lamps would l...</td>\n      <td>after early night fall the yellow lamps would ...</td>\n      <td>0.096154</td>\n      <td>0.333333</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>hello bertie any good in your mind</td>\n      <td>her about he and he good in your mind</td>\n      <td>0.352941</td>\n      <td>0.714286</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>number ten fresh nelly is waiting on you good ...</td>\n      <td>none but den fresh now as waiting on you could...</td>\n      <td>0.254237</td>\n      <td>0.545455</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"reference_values = [\n    {\n        \"gt\": \"he hoped there would be stew for dinner turnips and carrots and bruised potatoes and fat mutton pieces to be ladled out in thick peppered flour fattened sauce\",\n        \"prediction\": \"he hoped there would be stew for dinner turnips and characts and bruised potatoes and fat much and pieces to be lateled out in the thick peppered flowerfacton sauce\"\n    },\n    {\n        \"gt\": \"stuff it into you his belly counselled him\",\n        \"prediction\": \"stuffed into you his belly counciled him\"\n    },\n    {\n        \"gt\": \"after early nightfall the yellow lamps would light up here and there the squalid quarter of the brothels\",\n        \"prediction\": \"after early night fall the yellow lamps would lie how peer and there the squalit quarter of the brothels\"\n    },\n    {\n        \"gt\": \"hello bertie any good in your mind\",\n        \"prediction\": \"her about he and he good in your mind\"\n    },\n    {\n        \"gt\": \"number ten fresh nelly is waiting on you good night husband\",\n        \"prediction\": \"none but den fresh now as waiting on you could night husband\"\n    }\n]\n\n","metadata":{"id":"_Udw1t9ii-k8","execution":{"iopub.status.busy":"2023-04-26T00:50:52.988747Z","iopub.execute_input":"2023-04-26T00:50:52.989262Z","iopub.status.idle":"2023-04-26T00:50:52.996384Z","shell.execute_reply.started":"2023-04-26T00:50:52.989220Z","shell.execute_reply":"2023-04-26T00:50:52.995089Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"for index in range(5):\n    gt = predictions.iloc[index].ground_truth\n    prediction = predictions.iloc[index].prediction\n    assert gt == reference_values[index][\"gt\"]\n    assert prediction == reference_values[index][\"prediction\"]","metadata":{"id":"g7Pk9s7ei-k8","execution":{"iopub.status.busy":"2023-04-26T00:50:52.998250Z","iopub.execute_input":"2023-04-26T00:50:52.998986Z","iopub.status.idle":"2023-04-26T00:50:53.022342Z","shell.execute_reply.started":"2023-04-26T00:50:52.998946Z","shell.execute_reply":"2023-04-26T00:50:53.021250Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"## Train your model (10 points)","metadata":{"id":"rKCR9BdFi-k9"}},{"cell_type":"markdown","source":"Here you can launch training of the model you've just built. To get **4 points**, provide the curves for test loss, CER and WER from Weights & Biases.\n\nAfter training, you will get the test metric values on the hold-out test set. To get the rest **6 points**, try to pass the following thresholds:\n\n- 0.15 test CER\n- 0.3 test WER","metadata":{"id":"MBEOtdeZi-k9"}},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2023-04-26T00:09:30.543678Z","iopub.execute_input":"2023-04-26T00:09:30.544813Z","iopub.status.idle":"2023-04-26T00:09:31.553796Z","shell.execute_reply.started":"2023-04-26T00:09:30.544770Z","shell.execute_reply":"2023-04-26T00:09:31.552526Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Wed Apr 26 00:09:31 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   30C    P0    26W / 250W |      2MiB / 16280MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"!nvcc --version","metadata":{"execution":{"iopub.status.busy":"2023-04-26T00:09:31.555934Z","iopub.execute_input":"2023-04-26T00:09:31.556594Z","iopub.status.idle":"2023-04-26T00:09:32.533996Z","shell.execute_reply.started":"2023-04-26T00:09:31.556546Z","shell.execute_reply":"2023-04-26T00:09:32.532706Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"nvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2021 NVIDIA Corporation\nBuilt on Mon_May__3_19:15:13_PDT_2021\nCuda compilation tools, release 11.3, V11.3.109\nBuild cuda_11.3.r11.3/compiler.29920130_0\n","output_type":"stream"}]},{"cell_type":"code","source":"def train(model: nn.Module, device: str, train_loader: data.DataLoader,\n          test_sample: List[torch.Tensor], criterion: nn.Module, optimizer: \n          torch.optim.Optimizer, epoch: int, eval_period: int = 100) -> None:\n    \"\"\"\n    :param model: an RNN-T model\n    :param device: \"gpu\" or \"cpu\"\n    :param train_loader: training data loader\n    :param test_sample: a sample from the test set to log preliminary inference metrics\n    :param criterion: the loss function\n    :param optimizer: the training optimizer\n    :param epoch: the current epoch number\n    :param eval_period: the number of iterations between evaluations\n    \"\"\"\n    model.train()\n    data_len = len(train_loader.dataset)\n\n    for batch_idx, _data in tqdm(enumerate(train_loader), total=data_len):\n        spectrograms, labels, input_lengths, label_lengths = _data\n        spectrograms, labels = spectrograms.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n\n        output = model.forward(spectrograms, input_lengths, labels, label_lengths)   # (batch, time, label_length, n_class)\n        output = F.log_softmax(output, dim=-1)\n        \n        loss = criterion(\n            output, \n            labels, \n            input_lengths.to(device), \n            label_lengths.to(device)\n        )\n        loss.backward()\n        optimizer.step()\n        \n        if batch_idx % eval_period == 0 or batch_idx == data_len:\n            wandb.log({'loss_train': loss.item()})\n            \n            with torch.no_grad():\n                spectrograms, labels, input_lengths, label_lengths = test_sample\n                spectrograms, labels = spectrograms.to(device), labels.to(device)\n                predictions = get_transducer_predictions(\n                    model, spectrograms, input_lengths, \n                    labels, label_lengths\n                )\n                output = model.forward(spectrograms, input_lengths, labels, label_lengths)\n                val_loss = criterion(\n                  output, \n                  labels, \n                  input_lengths.to(device), \n                  label_lengths.to(device)\n                )\n                wandb.log({'loss_val': val_loss.item()})\n                clear_output(wait=True)\n                print('\\nTrain Epoch: {} [{}/{} ({:.0f}%)]\\tTrain Loss: {:.6f}\\tVal loss: {:.6f}'.format(\n                      epoch, batch_idx * len(spectrograms), data_len,\n                      100. * batch_idx / len(train_loader), loss.item(), val_loss.item()))\n                print(f\"cer: {predictions.cer.mean()}, wer: {predictions.wer.mean()}\")\n                display(predictions)\n                wandb.log({'cer_val': predictions.cer.mean()})\n                wandb.log({'wer_val': predictions.wer.mean()})\n                wandb.log({'val_predictions': wandb.Table(dataframe=predictions)})\n\n\ndef test(model: nn.Module, device: str, test_loader: data.DataLoader, \n         criterion: nn.Module, epoch: int, total_steps: int = None, \n         log_predictions: bool = False) -> None:\n    \"\"\"\n    :param model: an RNN-T model\n    :param device: \"gpu\" or \"cpu\"\n    :param test_loader: test data loader\n    :param criterion: the loss function\n    :param epoch: the current epoch number\n    :param total_steps: the number of test steps to perform. If None, the whole test set will be used for evaluation\n    :param log_predictions: if True, the predicted labels will be logged to the W&B dashboard\n    \"\"\"\n    print('Beginning eval...')\n    model.eval()\n    test_cer, test_wer, test_loss = [], [], []\n    test_predictions = []\n    if total_steps is None:\n        total_steps = len(test_loader)\n        \n    with torch.no_grad():\n        for i, _data in tqdm_notebook(enumerate(test_loader), total=total_steps):\n            if i == total_steps:\n                break\n            spectrograms, labels, input_lengths, label_lengths = _data\n            spectrograms, labels = spectrograms.to(device), labels.to(device)\n            output = model.forward(spectrograms, input_lengths, labels, label_lengths)\n            loss = criterion(\n              output, \n              labels, \n              input_lengths.to(device), \n              label_lengths.to(device)\n            )\n            test_loss.append(loss.item())\n            \n            predictions = get_transducer_predictions(\n                model, spectrograms, input_lengths, \n                labels, label_lengths\n            )\n            test_cer += list(predictions.cer)\n            test_wer += list(predictions.wer)\n            if log_predictions:\n                test_predictions.append(predictions)\n\n    avg_cer = np.mean(test_cer)\n    avg_wer = np.mean(test_wer)\n    avg_loss = np.mean(test_loss)\n    \n    if total_steps < len(test_loader):\n        wandb.log({\n            'loss_test': avg_loss, \n            'avg_cer': avg_cer, \n            'avg_wer': avg_wer\n        })\n    else:\n        wandb.log({\n            'loss_test_final': avg_loss, \n            'avg_cer_final': avg_cer, \n            'avg_wer_final': avg_wer\n        })\n    if log_predictions:\n        wandb.log({'test_predictions': wandb.Table(dataframe=pd.concat(test_predictions, ignore_index=True))})\n        \n    print('Epoch: {:d}, Test set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'.format(\n        epoch, avg_loss, avg_cer, avg_wer))\n    ","metadata":{"id":"6wyqZtQ8i-k9","execution":{"iopub.status.busy":"2023-04-26T00:50:56.918887Z","iopub.execute_input":"2023-04-26T00:50:56.919315Z","iopub.status.idle":"2023-04-26T00:50:56.940606Z","shell.execute_reply.started":"2023-04-26T00:50:56.919277Z","shell.execute_reply":"2023-04-26T00:50:56.939396Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"torch.manual_seed(7)\nif torch.cuda.is_available():\n    print('GPU found! 🎉')\n    device = 'cuda'\nelse:\n    print('Only CPU found! 💻')\n    device = 'cpu'\n\n# Hyperparameters for your model\n\nhparams = {\n    'model': {\n        'num_classes': len(tokenizer.char_map),\n        'input_dim': 80,\n        'num_encoder_layers': 4,\n        'num_decoder_layers': 1,\n        'encoder_hidden_state_dim': 320,\n        'decoder_hidden_state_dim': 512,\n        'output_dim': 512,\n        'encoder_is_bidirectional': True,\n        'encoder_dropout_p': 0.2,\n        'decoder_dropout_p': 0.2\n    },\n    'data': {\n        'batch_size': 1,\n        'epochs': 10,\n        'learning_rate': 1e-4\n    }\n}\n\nkwargs = {'num_workers': 1, 'pin_memory': True} if device == 'cuda' else {}\ntrain_loader = data.DataLoader(train_dataset, batch_size=hparams['data']['batch_size'], \n                               shuffle=True, collate_fn=lambda x: data_processing(x), **kwargs)\ntest_loader = data.DataLoader(test_dataset, batch_size=hparams['data']['batch_size'], \n                              shuffle=False, collate_fn=lambda x: data_processing(x, 'test'), **kwargs)\n\n","metadata":{"id":"lOWGMWf9i-k-","outputId":"4bb03d7e-413f-4571-b024-618169146b8a","execution":{"iopub.status.busy":"2023-04-26T00:50:59.609025Z","iopub.execute_input":"2023-04-26T00:50:59.610119Z","iopub.status.idle":"2023-04-26T00:50:59.631959Z","shell.execute_reply.started":"2023-04-26T00:50:59.610062Z","shell.execute_reply":"2023-04-26T00:50:59.630559Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"GPU found! 🎉\n","output_type":"stream"}]},{"cell_type":"code","source":"model = RNNTransducer(**hparams['model'])\nmodel.to(device)","metadata":{"id":"RPzsOxuai-k-","outputId":"1f20306a-d430-4ca8-8a71-97fbb052c5ef","execution":{"iopub.status.busy":"2023-04-26T00:51:01.648129Z","iopub.execute_input":"2023-04-26T00:51:01.649158Z","iopub.status.idle":"2023-04-26T00:51:08.630921Z","shell.execute_reply.started":"2023-04-26T00:51:01.649106Z","shell.execute_reply":"2023-04-26T00:51:08.629805Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n  \"num_layers={}\".format(dropout, num_layers))\n","output_type":"stream"},{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"RNNTransducer(\n  (encoder): EncoderRNNT(\n    (lstm): LSTM(80, 320, num_layers=4, batch_first=True, dropout=0.2, bidirectional=True)\n    (output_proj): Linear(in_features=640, out_features=512, bias=True)\n  )\n  (decoder): DecoderRNNT(\n    (embedding): Embedding(30, 512)\n    (lstm): LSTM(512, 512, batch_first=True, dropout=0.2)\n    (output_proj): Linear(in_features=512, out_features=512, bias=True)\n  )\n  (joiner): Joiner(\n    (linear): Linear(in_features=512, out_features=30, bias=True)\n    (relu): ReLU()\n    (softmax): Softmax(dim=-1)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"wandb.init(project=\"speech-transducer\", \n           group=\"base-architecture\",\n           config=hparams)","metadata":{"id":"0Y00qHnwi-k_","execution":{"iopub.status.busy":"2023-04-26T00:09:36.635299Z","iopub.execute_input":"2023-04-26T00:09:36.635677Z","iopub.status.idle":"2023-04-26T00:10:52.240537Z","shell.execute_reply.started":"2023-04-26T00:09:36.635622Z","shell.execute_reply":"2023-04-26T00:10:52.239481Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.15.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.14.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20230426_001020-aw5k7r47</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/eksolodneva/speech-transducer/runs/aw5k7r47' target=\"_blank\">fiery-river-3</a></strong> to <a href='https://wandb.ai/eksolodneva/speech-transducer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/eksolodneva/speech-transducer' target=\"_blank\">https://wandb.ai/eksolodneva/speech-transducer</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/eksolodneva/speech-transducer/runs/aw5k7r47' target=\"_blank\">https://wandb.ai/eksolodneva/speech-transducer/runs/aw5k7r47</a>"},"metadata":{}},{"execution_count":32,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/eksolodneva/speech-transducer/runs/aw5k7r47?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x72c4f1a4a890>"},"metadata":{}}]},{"cell_type":"code","source":"pip uninstall torch torchvision torchaudio -y","metadata":{"execution":{"iopub.status.busy":"2023-04-26T00:53:13.970964Z","iopub.execute_input":"2023-04-26T00:53:13.972056Z","iopub.status.idle":"2023-04-26T00:53:52.007313Z","shell.execute_reply.started":"2023-04-26T00:53:13.971987Z","shell.execute_reply":"2023-04-26T00:53:52.005930Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"Found existing installation: torch 1.13.0\nUninstalling torch-1.13.0:\n  Successfully uninstalled torch-1.13.0\nFound existing installation: torchvision 0.14.0\nUninstalling torchvision-0.14.0:\n  Successfully uninstalled torchvision-0.14.0\nFound existing installation: torchaudio 0.13.0\nUninstalling torchaudio-0.13.0:\n  Successfully uninstalled torchaudio-0.13.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu113","metadata":{"execution":{"iopub.status.busy":"2023-04-26T00:53:52.010340Z","iopub.execute_input":"2023-04-26T00:53:52.010776Z","iopub.status.idle":"2023-04-26T00:55:37.698645Z","shell.execute_reply.started":"2023-04-26T00:53:52.010731Z","shell.execute_reply":"2023-04-26T00:55:37.697120Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu113\nCollecting torch==1.12.1+cu113\n  Downloading https://download.pytorch.org/whl/cu113/torch-1.12.1%2Bcu113-cp37-cp37m-linux_x86_64.whl (1837.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 GB\u001b[0m \u001b[31m597.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:02\u001b[0mm\n\u001b[?25hCollecting torchvision==0.13.1+cu113\n  Downloading https://download.pytorch.org/whl/cu113/torchvision-0.13.1%2Bcu113-cp37-cp37m-linux_x86_64.whl (23.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.4/23.4 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting torchaudio==0.12.1\n  Downloading https://download.pytorch.org/whl/cu113/torchaudio-0.12.1%2Bcu113-cp37-cp37m-linux_x86_64.whl (3.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.12.1+cu113) (4.4.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchvision==0.13.1+cu113) (2.28.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision==0.13.1+cu113) (1.21.6)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision==0.13.1+cu113) (9.4.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision==0.13.1+cu113) (2.1.1)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision==0.13.1+cu113) (2022.12.7)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision==0.13.1+cu113) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision==0.13.1+cu113) (1.26.14)\nInstalling collected packages: torch, torchvision, torchaudio\nSuccessfully installed torch-1.12.1+cu113 torchaudio-0.12.1+cu113 torchvision-0.13.1+cu113\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install torch==1.8.0+cu111 torchvision==0.9.0+cu111 torchaudio==0.8.0 -f https://download.pytorch.org/whl/torch_stable.html","metadata":{"execution":{"iopub.status.busy":"2023-04-26T00:11:15.262055Z","iopub.execute_input":"2023-04-26T00:11:15.262474Z","iopub.status.idle":"2023-04-26T00:13:07.792992Z","shell.execute_reply.started":"2023-04-26T00:11:15.262426Z","shell.execute_reply":"2023-04-26T00:13:07.791620Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Looking in links: https://download.pytorch.org/whl/torch_stable.html\nCollecting torch==1.8.0+cu111\n  Downloading https://download.pytorch.org/whl/cu111/torch-1.8.0%2Bcu111-cp37-cp37m-linux_x86_64.whl (1982.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 GB\u001b[0m \u001b[31m555.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:01\u001b[0mm00:01\u001b[0m\n\u001b[?25hCollecting torchvision==0.9.0+cu111\n  Downloading https://download.pytorch.org/whl/cu111/torchvision-0.9.0%2Bcu111-cp37-cp37m-linux_x86_64.whl (17.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting torchaudio==0.8.0\n  Downloading torchaudio-0.8.0-cp37-cp37m-manylinux1_x86_64.whl (1.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.8.0+cu111) (4.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch==1.8.0+cu111) (1.21.6)\nRequirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from torchvision==0.9.0+cu111) (9.4.0)\nInstalling collected packages: torch, torchvision, torchaudio\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorchmetrics 0.11.4 requires torch>=1.8.1, but you have torch 1.8.0+cu111 which is incompatible.\npytorch-lightning 1.9.4 requires torch>=1.10.0, but you have torch 1.8.0+cu111 which is incompatible.\nkornia 0.6.11 requires torch>=1.9.1, but you have torch 1.8.0+cu111 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed torch-1.8.0+cu111 torchaudio-0.8.0 torchvision-0.9.0+cu111\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html","metadata":{"execution":{"iopub.status.busy":"2023-04-26T00:13:25.565110Z","iopub.execute_input":"2023-04-26T00:13:25.565519Z","iopub.status.idle":"2023-04-26T00:14:41.388954Z","shell.execute_reply.started":"2023-04-26T00:13:25.565481Z","shell.execute_reply":"2023-04-26T00:14:41.387148Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"Looking in links: https://download.pytorch.org/whl/torch_stable.html\nCollecting torch==1.7.1+cu110\n  Downloading https://download.pytorch.org/whl/cu110/torch-1.7.1%2Bcu110-cp37-cp37m-linux_x86_64.whl (1156.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 GB\u001b[0m \u001b[31m805.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting torchvision==0.8.2+cu110\n  Downloading https://download.pytorch.org/whl/cu110/torchvision-0.8.2%2Bcu110-cp37-cp37m-linux_x86_64.whl (12.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting torchaudio==0.7.2\n  Downloading torchaudio-0.7.2-cp37-cp37m-manylinux1_x86_64.whl (7.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch==1.7.1+cu110) (1.21.6)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.7.1+cu110) (4.4.0)\nRequirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from torchvision==0.8.2+cu110) (9.4.0)\nInstalling collected packages: torch, torchvision, torchaudio\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorchmetrics 0.11.4 requires torch>=1.8.1, but you have torch 1.7.1+cu110 which is incompatible.\npytorch-lightning 1.9.4 requires torch>=1.10.0, but you have torch 1.7.1+cu110 which is incompatible.\nkornia 0.6.11 requires torch>=1.9.1, but you have torch 1.7.1+cu110 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed torch-1.7.1+cu110 torchaudio-0.7.2 torchvision-0.8.2+cu110\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio==0.8.1 -f https://download.pytorch.org/whl/torch_stable.html","metadata":{"execution":{"iopub.status.busy":"2023-04-26T00:14:55.802848Z","iopub.execute_input":"2023-04-26T00:14:55.804131Z","iopub.status.idle":"2023-04-26T00:16:49.658150Z","shell.execute_reply.started":"2023-04-26T00:14:55.804080Z","shell.execute_reply":"2023-04-26T00:16:49.656733Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"Looking in links: https://download.pytorch.org/whl/torch_stable.html\nCollecting torch==1.8.1+cu111\n  Downloading https://download.pytorch.org/whl/cu111/torch-1.8.1%2Bcu111-cp37-cp37m-linux_x86_64.whl (1982.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 GB\u001b[0m \u001b[31m540.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting torchvision==0.9.1+cu111\n  Downloading https://download.pytorch.org/whl/cu111/torchvision-0.9.1%2Bcu111-cp37-cp37m-linux_x86_64.whl (17.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting torchaudio==0.8.1\n  Downloading torchaudio-0.8.1-cp37-cp37m-manylinux1_x86_64.whl (1.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch==1.8.1+cu111) (1.21.6)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.8.1+cu111) (4.4.0)\nRequirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from torchvision==0.9.1+cu111) (9.4.0)\nInstalling collected packages: torch, torchvision, torchaudio\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npytorch-lightning 1.9.4 requires torch>=1.10.0, but you have torch 1.8.1+cu111 which is incompatible.\nkornia 0.6.11 requires torch>=1.9.1, but you have torch 1.8.1+cu111 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed torch-1.8.1+cu111 torchaudio-0.8.1 torchvision-0.9.1+cu111\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install torch==1.8.2 torchvision==0.9.2 torchaudio==0.8.2 --extra-index-url https://download.pytorch.org/whl/lts/1.8/cu111","metadata":{"execution":{"iopub.status.busy":"2023-04-26T00:17:56.109437Z","iopub.execute_input":"2023-04-26T00:17:56.109881Z","iopub.status.idle":"2023-04-26T00:21:16.933713Z","shell.execute_reply.started":"2023-04-26T00:17:56.109839Z","shell.execute_reply":"2023-04-26T00:21:16.932270Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/lts/1.8/cu111\nCollecting torch==1.8.2\n  Downloading https://download.pytorch.org/whl/lts/1.8/cu111/torch-1.8.2%2Bcu111-cp37-cp37m-linux_x86_64.whl (1982.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 GB\u001b[0m \u001b[31m575.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:01\u001b[0m00:04\u001b[0m\n\u001b[?25hCollecting torchvision==0.9.2\n  Downloading https://download.pytorch.org/whl/lts/1.8/cu111/torchvision-0.9.2%2Bcu111-cp37-cp37m-linux_x86_64.whl (17.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.5/17.5 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting torchaudio==0.8.2\n  Downloading https://download.pytorch.org/whl/lts/1.8/torchaudio-0.8.2-cp37-cp37m-linux_x86_64.whl (1.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch==1.8.2) (1.21.6)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.8.2) (4.4.0)\nRequirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from torchvision==0.9.2) (9.4.0)\nInstalling collected packages: torch, torchvision, torchaudio\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npytorch-lightning 1.9.4 requires torch>=1.10.0, but you have torch 1.8.2+cu111 which is incompatible.\nkornia 0.6.11 requires torch>=1.9.1, but you have torch 1.8.2+cu111 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed torch-1.8.2+cu111 torchaudio-0.8.2 torchvision-0.9.2+cu111\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install torch==1.9.1+cu111 torchvision==0.10.1+cu111 torchaudio==0.9.1 -f https://download.pytorch.org/whl/torch_stable.html","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install torch==1.10.0+cu111 torchvision==0.11.0+cu111 torchaudio==0.10.0 -f https://download.pytorch.org/whl/torch_stable.html","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install torch==1.10.1+cu111 torchvision==0.11.2+cu111 torchaudio==0.10.1 -f https://download.pytorch.org/whl/cu111/torch_stable.html","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=hparams['data']['learning_rate'])\ncriterion = RNNTLoss(blank=tokenizer.get_symbol_index(BLANK_SYMBOL), reduction='mean')\ntest_sample = next(iter(test_loader))\n\nfor epoch in notebook.tqdm(range(1, hparams['data']['epochs'] + 1)):\n    train(model, device, train_loader, test_sample, criterion, optimizer, epoch, eval_period=50)\n    utils.save_checkpoint(model, checkpoint_name=f'model_epoch{epoch}.tar', path=snapshot_dir)\n    wandb.save(f'model_epoch{epoch}.tar')\n    test(model, device, test_loader, criterion, epoch, total_steps=20, log_predictions=True)\n\nutils.save_checkpoint(model, checkpoint_name=f'model.tar')","metadata":{"id":"A6HlgxA9i-k_","execution":{"iopub.status.busy":"2023-04-26T00:55:37.700691Z","iopub.execute_input":"2023-04-26T00:55:37.701028Z","iopub.status.idle":"2023-04-26T00:55:38.203551Z","shell.execute_reply.started":"2023-04-26T00:55:37.700980Z","shell.execute_reply":"2023-04-26T00:55:38.200998Z"},"trusted":true},"execution_count":45,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e62bb7813e694aa181450c4bf94323d6"}},"metadata":{}},{"name":"stderr","text":"\n  0%|          | 0/28539 [00:00<?, ?it/s]\u001b[A\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/70796330.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnotebook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_period\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'model_epoch{epoch}.tar'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msnapshot_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'model_epoch{epoch}.tar'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_23/1803905585.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, test_sample, criterion, optimizer, epoch, eval_period)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0minput_lengths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mlabel_lengths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         )\n\u001b[1;32m     32\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_state_dict_post_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'_non_persistent_buffers_set'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_non_persistent_buffers_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'_is_full_backward_hook'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_full_backward_hook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchaudio/transforms/_transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, logits, targets, logit_lengths, target_lengths)\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchaudio/functional/functional.py\u001b[0m in \u001b[0;36mrnnt_loss\u001b[0;34m(logits, targets, logit_lengths, target_lengths, blank, clamp, reduction)\u001b[0m\n\u001b[1;32m   1877\u001b[0m             \u001b[0mTensor\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mdimensions\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1878\u001b[0m         \u001b[0mpsd_n\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mcomplex\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mvalued\u001b[0m \u001b[0mpower\u001b[0m \u001b[0mspectral\u001b[0m \u001b[0mdensity\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mPSD\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mmatrix\u001b[0m \u001b[0mof\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1879\u001b[0;31m             \u001b[0mTensor\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mdimensions\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1880\u001b[0m     \"\"\"\n\u001b[1;32m   1881\u001b[0m     assert (\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n","\u001b[0;31mNotImplementedError\u001b[0m: Could not run 'torchaudio::rnnt_loss' with arguments from the 'CUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'torchaudio::rnnt_loss' is only available for these backends: [CPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/cpu/compute.cpp:142 [kernel]\nBackendSelect: fallthrough registered at /usr/local/src/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /usr/local/src/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /usr/local/src/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /usr/local/src/pytorch/aten/src/ATen/FunctionalizeFallbackKernel.cpp:291 [backend fallback]\nNamed: registered at /usr/local/src/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /usr/local/src/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /usr/local/src/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /usr/local/src/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at /usr/local/src/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nAutogradCPU: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nAutogradCUDA: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nAutogradHIP: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nAutogradXLA: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nAutogradMPS: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nAutogradIPU: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nAutogradXPU: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nAutogradHPU: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nAutogradVE: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nAutogradLazy: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nAutogradMeta: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nAutogradPrivateUse1: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nAutogradPrivateUse2: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nAutogradPrivateUse3: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nAutogradNestedTensor: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nTracer: registered at /usr/local/src/pytorch/torch/csrc/autograd/TraceTypeManual.cpp:296 [backend fallback]\nAutocastCPU: fallthrough registered at /usr/local/src/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastCUDA: fallthrough registered at /usr/local/src/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /usr/local/src/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /usr/local/src/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /usr/local/src/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /usr/local/src/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /usr/local/src/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /usr/local/src/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /usr/local/src/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /usr/local/src/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"],"ename":"NotImplementedError","evalue":"Could not run 'torchaudio::rnnt_loss' with arguments from the 'CUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'torchaudio::rnnt_loss' is only available for these backends: [CPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/cpu/compute.cpp:142 [kernel]\nBackendSelect: fallthrough registered at /usr/local/src/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /usr/local/src/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /usr/local/src/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /usr/local/src/pytorch/aten/src/ATen/FunctionalizeFallbackKernel.cpp:291 [backend fallback]\nNamed: registered at /usr/local/src/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /usr/local/src/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /usr/local/src/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /usr/local/src/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at /usr/local/src/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nAutogradCPU: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nAutogradCUDA: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nAutogradHIP: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nAutogradXLA: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nAutogradMPS: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nAutogradIPU: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nAutogradXPU: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nAutogradHPU: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nAutogradVE: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nAutogradLazy: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nAutogradMeta: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nAutogradPrivateUse1: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nAutogradPrivateUse2: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nAutogradPrivateUse3: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nAutogradNestedTensor: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nTracer: registered at /usr/local/src/pytorch/torch/csrc/autograd/TraceTypeManual.cpp:296 [backend fallback]\nAutocastCPU: fallthrough registered at /usr/local/src/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastCUDA: fallthrough registered at /usr/local/src/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /usr/local/src/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /usr/local/src/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /usr/local/src/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /usr/local/src/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /usr/local/src/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /usr/local/src/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /usr/local/src/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /usr/local/src/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n","output_type":"error"}]},{"cell_type":"code","source":"test(model, device, test_loader, criterion, epoch)","metadata":{"id":"Dao_2Mj7i-lA","execution":{"iopub.status.busy":"2023-04-25T23:05:54.739732Z","iopub.execute_input":"2023-04-25T23:05:54.740807Z","iopub.status.idle":"2023-04-25T23:05:55.075712Z","shell.execute_reply.started":"2023-04-25T23:05:54.740765Z","shell.execute_reply":"2023-04-25T23:05:55.072008Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"Beginning eval...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:84: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2620 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b333e9c9dfc948a094b439e05f309f23"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/1263675344.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_23/1803905585.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model, device, test_loader, criterion, epoch, total_steps, log_predictions)\u001b[0m\n\u001b[1;32m     92\u001b[0m               \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m               \u001b[0minput_lengths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m               \u001b[0mlabel_lengths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m             )\n\u001b[1;32m     96\u001b[0m             \u001b[0mtest_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchaudio/transforms/_transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, logits, targets, logit_lengths, target_lengths)\u001b[0m\n\u001b[1;32m   1788\u001b[0m             \u001b[0motherwise\u001b[0m \u001b[0mscalar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1789\u001b[0m         \"\"\"\n\u001b[0;32m-> 1790\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnnt_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogit_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchaudio/functional/functional.py\u001b[0m in \u001b[0;36mrnnt_loss\u001b[0;34m(logits, targets, logit_lengths, target_lengths, blank, clamp, reduction)\u001b[0m\n\u001b[1;32m   1877\u001b[0m         \u001b[0mtarget_lengths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_lengths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1878\u001b[0m         \u001b[0mblank\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mblank\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1879\u001b[0;31m         \u001b[0mclamp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1880\u001b[0m     )\n\u001b[1;32m   1881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;31m# We save the function ptr as the `op` attribute on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0;31m# OpOverloadPacket to access it here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[0;31m# TODO: use this to make a __dir__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotImplementedError\u001b[0m: Could not run 'torchaudio::rnnt_loss' with arguments from the 'CUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'torchaudio::rnnt_loss' is only available for these backends: [CPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/cpu/compute.cpp:142 [kernel]\nBackendSelect: fallthrough registered at /usr/local/src/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /usr/local/src/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /usr/local/src/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /usr/local/src/pytorch/aten/src/ATen/FunctionalizeFallbackKernel.cpp:291 [backend fallback]\nNamed: registered at /usr/local/src/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /usr/local/src/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /usr/local/src/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /usr/local/src/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at /usr/local/src/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nAutogradCPU: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nAutogradCUDA: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nAutogradHIP: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nAutogradXLA: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nAutogradMPS: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nAutogradIPU: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nAutogradXPU: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nAutogradHPU: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nAutogradVE: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nAutogradLazy: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nAutogradMeta: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nAutogradPrivateUse1: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nAutogradPrivateUse2: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nAutogradPrivateUse3: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nAutogradNestedTensor: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nTracer: registered at /usr/local/src/pytorch/torch/csrc/autograd/TraceTypeManual.cpp:296 [backend fallback]\nAutocastCPU: fallthrough registered at /usr/local/src/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastCUDA: fallthrough registered at /usr/local/src/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /usr/local/src/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /usr/local/src/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /usr/local/src/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /usr/local/src/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /usr/local/src/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /usr/local/src/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /usr/local/src/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /usr/local/src/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"],"ename":"NotImplementedError","evalue":"Could not run 'torchaudio::rnnt_loss' with arguments from the 'CUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'torchaudio::rnnt_loss' is only available for these backends: [CPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/cpu/compute.cpp:142 [kernel]\nBackendSelect: fallthrough registered at /usr/local/src/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /usr/local/src/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /usr/local/src/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /usr/local/src/pytorch/aten/src/ATen/FunctionalizeFallbackKernel.cpp:291 [backend fallback]\nNamed: registered at /usr/local/src/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /usr/local/src/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /usr/local/src/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /usr/local/src/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at /usr/local/src/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nAutogradCPU: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nAutogradCUDA: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nAutogradHIP: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nAutogradXLA: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nAutogradMPS: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nAutogradIPU: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nAutogradXPU: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nAutogradHPU: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nAutogradVE: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nAutogradLazy: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nAutogradMeta: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nAutogradPrivateUse1: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nAutogradPrivateUse2: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nAutogradPrivateUse3: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nAutogradNestedTensor: registered at /usr/local/src/audio/torchaudio/csrc/rnnt/autograd.cpp:51 [autograd kernel]\nTracer: registered at /usr/local/src/pytorch/torch/csrc/autograd/TraceTypeManual.cpp:296 [backend fallback]\nAutocastCPU: fallthrough registered at /usr/local/src/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastCUDA: fallthrough registered at /usr/local/src/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /usr/local/src/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /usr/local/src/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /usr/local/src/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /usr/local/src/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /usr/local/src/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /usr/local/src/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /usr/local/src/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /usr/local/src/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
