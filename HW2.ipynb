{"metadata":{"colab":{"collapsed_sections":["lnUmkwSbY0Ho","qg4JIKs4EYtQ","GLRub0wZY0Hs","Zc1A7_xcwiT5","3cP79coczzan","hI6wj5hrz9vJ","txfmuahS0Apx","1nu4ISUMGt-J","AOSUt5iox8B6","TULzSmkZxgEn","PfZPlnRjjmOJ","oe_g0pXw21Tn","gB79UDNM8UOZ"],"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"pycharm":{"stem_cell":{"cell_type":"raw","metadata":{"collapsed":false},"source":[]}},"widgets":{"application/vnd.jupyter.widget-state+json":{"ad2bb120428c441c885cada8939f365b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8a74d3f38d6a4f21baf634ac16c250ff","IPY_MODEL_f2b1c496139943cb94a32ab4bee64861","IPY_MODEL_e68b23df39d44709a7264a1b984202ce"],"layout":"IPY_MODEL_349540f3959540cfa9e37817fe6fb87a"}},"8a74d3f38d6a4f21baf634ac16c250ff":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d105d106485d4e6fb293abb0ccfe1d50","placeholder":"​","style":"IPY_MODEL_2c2c832b70b343ec9d02eb78a49012cd","value":"100%"}},"f2b1c496139943cb94a32ab4bee64861":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_52a0c4da6c72493081c269b54bd2d34a","max":6387309499,"min":0,"orientation":"horizontal","style":"IPY_MODEL_187e1adc87184c40aaf7e0e42c9fc12f","value":6387309499}},"e68b23df39d44709a7264a1b984202ce":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5f47531ac07c4c3eb93478200e5da284","placeholder":"​","style":"IPY_MODEL_3cdc8f0e143a4c11bbc5f221e59afb0c","value":" 5.95G/5.95G [02:06&lt;00:00, 54.6MB/s]"}},"349540f3959540cfa9e37817fe6fb87a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d105d106485d4e6fb293abb0ccfe1d50":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2c2c832b70b343ec9d02eb78a49012cd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"52a0c4da6c72493081c269b54bd2d34a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"187e1adc87184c40aaf7e0e42c9fc12f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5f47531ac07c4c3eb93478200e5da284":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3cdc8f0e143a4c11bbc5f221e59afb0c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3699c443d8384126b82546c8375f36c6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0409ea0806ee475398b017fd9c109e79","IPY_MODEL_e7069c91cc5d4d88904f4056df26760c","IPY_MODEL_5d040965274146b8a8af53a4e2d27914"],"layout":"IPY_MODEL_0c777e21e2874f4c98e5ac6761c645d2"}},"0409ea0806ee475398b017fd9c109e79":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cb951ee7c35141d083c45d0a1f3d187e","placeholder":"​","style":"IPY_MODEL_a59760e5be374f8e803cdfb5062243c9","value":"100%"}},"e7069c91cc5d4d88904f4056df26760c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1cbc864a06144a5f90548c3491d3f3a5","max":346663984,"min":0,"orientation":"horizontal","style":"IPY_MODEL_036f72d2c1a2478fa1ecf0856bc309ed","value":346663984}},"5d040965274146b8a8af53a4e2d27914":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6a4e80d8df644f54954cefbcfbd053b7","placeholder":"​","style":"IPY_MODEL_34fa8e6ce6f24daabecf30dcf1ec3fd1","value":" 331M/331M [00:03&lt;00:00, 86.6MB/s]"}},"0c777e21e2874f4c98e5ac6761c645d2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cb951ee7c35141d083c45d0a1f3d187e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a59760e5be374f8e803cdfb5062243c9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1cbc864a06144a5f90548c3491d3f3a5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"036f72d2c1a2478fa1ecf0856bc309ed":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6a4e80d8df644f54954cefbcfbd053b7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"34fa8e6ce6f24daabecf30dcf1ec3fd1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f03ca9c3cde54cc2ac450576dce0ab91":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_917c29ccb5bd4e8180c1cf6389687dea","IPY_MODEL_f891d5b9693249e5b268e2b5fc400f27","IPY_MODEL_29897923372345dfbc32db2bdffa5685"],"layout":"IPY_MODEL_cf49764871814a1796d804c114bb9fa7"}},"917c29ccb5bd4e8180c1cf6389687dea":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b077546f6441468fa8a9b3f8eb441175","placeholder":"​","style":"IPY_MODEL_7a82b7eec1d44128ac7f623dc246f4ec","value":"  0%"}},"f891d5b9693249e5b268e2b5fc400f27":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_814c65ce3221412080e16085af0f4db5","max":20,"min":0,"orientation":"horizontal","style":"IPY_MODEL_99d097ca76b84f2d8de69490144a2e87","value":0}},"29897923372345dfbc32db2bdffa5685":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cbe0e86841b8465dbc0db2a2a2546a43","placeholder":"​","style":"IPY_MODEL_7ca4f0a7009842489b23f4a20f5b6424","value":" 0/20 [00:00&lt;?, ?it/s]"}},"cf49764871814a1796d804c114bb9fa7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b077546f6441468fa8a9b3f8eb441175":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7a82b7eec1d44128ac7f623dc246f4ec":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"814c65ce3221412080e16085af0f4db5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"99d097ca76b84f2d8de69490144a2e87":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cbe0e86841b8465dbc0db2a2a2546a43":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7ca4f0a7009842489b23f4a20f5b6424":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8d1046874acf4d2e923ef8cccc996230":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_054174569ed84ac2b1d50b29c3cb233d","IPY_MODEL_59e3cccf8d4a4e91ae5c3179c53ecf4e","IPY_MODEL_fc3d39bf7b25494699cee8938619c63f"],"layout":"IPY_MODEL_6b96af8dcca3409a8435cf699ea1c101"}},"054174569ed84ac2b1d50b29c3cb233d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c40a1c55d934537b197b632b2237f5e","placeholder":"​","style":"IPY_MODEL_d6c3ba653c1a487bb4a9ba5120a87b68","value":" 50%"}},"59e3cccf8d4a4e91ae5c3179c53ecf4e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea4a8e1fbd2449e59b48eba4cccab134","max":20,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b5cbdbb9c60a4d40974a2c9e4254d06c","value":10}},"fc3d39bf7b25494699cee8938619c63f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_23292d20b0274432be17e73de63692b0","placeholder":"​","style":"IPY_MODEL_6c734cea188c47dd88787f917bc29c5a","value":" 10/20 [1:09:02&lt;1:08:52, 413.30s/it]"}},"6b96af8dcca3409a8435cf699ea1c101":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c40a1c55d934537b197b632b2237f5e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d6c3ba653c1a487bb4a9ba5120a87b68":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ea4a8e1fbd2449e59b48eba4cccab134":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b5cbdbb9c60a4d40974a2c9e4254d06c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"23292d20b0274432be17e73de63692b0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c734cea188c47dd88787f917bc29c5a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"gpuClass":"standard","accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Homework #1: train a CTC speech recognition model\n\nIn lecture you have examined the basics of speech recognition and covered the Connectionist Temporal Classification (CTC) model in detail. You are now ready to train your first \"adult\" speech recognition system!\n\nIn seminar 3 you implemented the CTC forward and backward algorithms in order to calculate the CTC loss and study the diffusion of probability in a CTC trellis. Also you implemented a greedy decoder and a prefix beam-search decoder\n\nIn this homework you will implement and train a CTC speech recognition model on a subset of the LibriSpeech corpus. This task will involve:\n\n- Creating a dataloader\n- Implementing a and training a Neural Network for CTC\n  * DNN\n  * LSTM\n  * BiLSTM\n- Comparing the Properties of these models","metadata":{"id":"EhkTsE_rVd-n"}},{"cell_type":"markdown","source":"# Setup - Install package, download files, etc...","metadata":{"id":"qg4JIKs4EYtQ"}},{"cell_type":"code","source":"# uncomment if needed. If you run the notebook in Colab, all these libraries are pre-installed\n# !pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\n# !pip install numpy==1.17.5 matplotlib==3.3.3 tqdm==4.54.0","metadata":{"id":"Gk1mFIiwve6R","execution":{"iopub.status.busy":"2023-03-19T10:03:05.285275Z","iopub.execute_input":"2023-03-19T10:03:05.285765Z","iopub.status.idle":"2023-03-19T10:03:05.316776Z","shell.execute_reply.started":"2023-03-19T10:03:05.285727Z","shell.execute_reply":"2023-03-19T10:03:05.315803Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install arpa","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VwuUeuXXBDPl","outputId":"4f1c9d16-46cb-432e-cc0d-a41d0e7a00e6","execution":{"iopub.status.busy":"2023-03-19T10:03:05.318793Z","iopub.execute_input":"2023-03-19T10:03:05.319481Z","iopub.status.idle":"2023-03-19T10:03:16.804073Z","shell.execute_reply.started":"2023-03-19T10:03:05.319444Z","shell.execute_reply":"2023-03-19T10:03:16.802822Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting arpa\n  Downloading arpa-0.1.0b4-py3-none-any.whl (9.6 kB)\nInstalling collected packages: arpa\nSuccessfully installed arpa-0.1.0b4\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"%%capture\n!pip install wandb -qqq","metadata":{"id":"HN6poM45AAXH","execution":{"iopub.status.busy":"2023-03-19T10:03:16.806890Z","iopub.execute_input":"2023-03-19T10:03:16.807287Z","iopub.status.idle":"2023-03-19T10:03:26.553891Z","shell.execute_reply.started":"2023-03-19T10:03:16.807240Z","shell.execute_reply":"2023-03-19T10:03:26.552488Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# comment out if needed\n# !mkdir week_03_files\n# !wget -O week_03_files/utils.py https://raw.githubusercontent.com/yandexdataschool/speech_course/main/week_05/utils.py\n# !wget -O week_03_files/test_matrix.txt -q https://raw.githubusercontent.com/yandexdataschool/speech_course/main/week_05/test_matrix.txt\n# !wget -O week_03_files/soft_alignment.txt -q https://raw.githubusercontent.com/yandexdataschool/speech_course/main/week_05/soft_alignment.txt\n# !wget -O week_03_files/test_decode.txt -q https://raw.githubusercontent.com/yandexdataschool/speech_course/main/week_05/test_decode.txt\n# !wget -O week_03_files/test_labels.txt -q https://raw.githubusercontent.com/yandexdataschool/speech_course/main/week_05/test_labels.txt\n# !wget -O week_03_files/3-gram.pruned.1e-7.arpa -q https://raw.githubusercontent.com/yandexdataschool/speech_course/main/week_05/3-gram.pruned.1e-7.arpa","metadata":{"id":"fxq-Jmp_Y293","execution":{"iopub.status.busy":"2023-03-19T10:03:26.558456Z","iopub.execute_input":"2023-03-19T10:03:26.558800Z","iopub.status.idle":"2023-03-19T10:03:26.564408Z","shell.execute_reply.started":"2023-03-19T10:03:26.558762Z","shell.execute_reply":"2023-03-19T10:03:26.562804Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#!L\nimport math\nimport os\nimport shutil\nimport string\nimport time\nfrom collections import defaultdict\nfrom typing import List, Tuple, TypeVar, Optional, Callable, Iterable\n\nimport arpa\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torchaudio\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.data as data\nimport wandb\nfrom matplotlib.colors import LogNorm\nfrom torch import optim\nfrom tqdm.notebook import tqdm\n\nimport sys","metadata":{"id":"IM01xHP7Y0Hi","execution":{"iopub.status.busy":"2023-03-19T10:03:26.567338Z","iopub.execute_input":"2023-03-19T10:03:26.568211Z","iopub.status.idle":"2023-03-19T10:03:29.705303Z","shell.execute_reply.started":"2023-03-19T10:03:26.568156Z","shell.execute_reply":"2023-03-19T10:03:29.704134Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/severilov/DL-Audio-AIMasters-Course.git\nmodule_path = os.path.abspath(os.path.join('/kaggle/working/DL-Audio-AIMasters-Course/homework/hw2/'))\nif module_path not in sys.path:\n    sys.path.append(module_path)\n\nimport utils","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UDgtYC5vYLWo","outputId":"bc0f47ec-0462-4b3a-d562-f994ab0cfc2a","execution":{"iopub.status.busy":"2023-03-19T10:03:29.707650Z","iopub.execute_input":"2023-03-19T10:03:29.708706Z","iopub.status.idle":"2023-03-19T10:03:34.168069Z","shell.execute_reply.started":"2023-03-19T10:03:29.708627Z","shell.execute_reply":"2023-03-19T10:03:34.166831Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Cloning into 'DL-Audio-AIMasters-Course'...\nremote: Enumerating objects: 155, done.\u001b[K\nremote: Counting objects: 100% (13/13), done.\u001b[K\nremote: Compressing objects: 100% (12/12), done.\u001b[K\nremote: Total 155 (delta 1), reused 9 (delta 1), pack-reused 142\u001b[K\nReceiving objects: 100% (155/155), 71.87 MiB | 34.65 MiB/s, done.\nResolving deltas: 100% (41/41), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Seminar 3 recap: CTC Forward-Backward Algorithm + Soft alignment","metadata":{"id":"GLRub0wZY0Hs"}},{"cell_type":"markdown","source":"## CTC Forward Algorithm","metadata":{"id":"1nu4ISUMGt-J"}},{"cell_type":"code","source":"# Helper functions\nBLANK_SYMBOL = \"_\"\n\nclass Tokenizer:\n    \"\"\"\n    Maps characters to integers and vice versa\n    \"\"\"\n    def __init__(self):\n        self.char_map = {}\n        self.index_map = {}\n        for i, ch in enumerate([\"'\", \" \"] + list(string.ascii_lowercase) + [BLANK_SYMBOL]):\n            self.char_map[ch] = i\n            self.index_map[i] = ch\n        \n    def text_to_indices(self, text: str) -> List[int]:\n        return [self.char_map[ch] for ch in text]\n\n    def indices_to_text(self, labels: List[int]) -> str:                                                                                                                                                                                                                                 \n        return \"\".join([self.index_map[i] for i in labels])\n    \n    def get_symbol_index(self, sym: str) -> int:\n        return self.char_map[sym]\n    \n\ntokenizer = Tokenizer()\n\nNEG_INF = -float(\"inf\")\n\n\ndef logsumexp(*args) -> float:\n    \"\"\"\n    Log-sum-exp trick for log-domain calculations\n    See for details: https://en.wikipedia.org/wiki/LogSumExp\n    \"\"\"\n    if all(a == NEG_INF for a in args):\n        return NEG_INF\n    a_max = max(args)\n    lsp = math.log(sum(math.exp(a - a_max) for a in args))\n    return a_max + lsp\n\n\ndef modify_sequence(sequence: List[int], blank_idx: int) -> List[int]:\n    \"\"\"\n    Modifies sequence which with START, END blanks and between each character\n    \"\"\"\n    modified_sequence = []\n    \n    for idx in sequence:\n        modified_sequence += [blank_idx, idx]\n        \n    modified_sequence.append(blank_idx)\n    return modified_sequence","metadata":{"id":"HgRpr3bjY0Hv","execution":{"iopub.status.busy":"2023-03-19T10:03:34.171781Z","iopub.execute_input":"2023-03-19T10:03:34.172088Z","iopub.status.idle":"2023-03-19T10:03:34.184260Z","shell.execute_reply.started":"2023-03-19T10:03:34.172056Z","shell.execute_reply":"2023-03-19T10:03:34.183145Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"#!L\n\ndef forward_algorithm(sequence: List[int], matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    :param sequence: a string converted to an index array by Tokenizer\n    :param matrix: A matrix of shape (K, T) with probability distributions over phonemes at each moment of time.\n    :return: the result of the forward pass of shape (2 * len(sequence) + 1, T)\n    \"\"\"\n    # Turn probs into log-probs\n    matrix = np.log(matrix)\n    \n    blank = tokenizer.get_symbol_index(BLANK_SYMBOL)\n    mod_sequence = modify_sequence(sequence, blank)\n\n    # Initialze\n    # (2L + 1) x T \n    alphas = np.full([len(mod_sequence), matrix.shape[1]], NEG_INF)\n\n    for t in range(matrix.shape[1]):\n        for s in range(len(mod_sequence)):\n            # First Step\n            ch = mod_sequence[s]\n            if t == 0:\n                if s != 0 and s != 1:\n                    alphas[s][t] = NEG_INF\n                else:\n                    alphas[s][t] = matrix[ch][t]\n                \n            # Upper diagonal zeros\n            elif s < alphas.shape[0] - 2 * (alphas.shape[1]-t)-1:# CONDITION\n                alphas[s][t] = NEG_INF\n            else:\n                # Need to do this stabily\n                if s == 0:\n                    alphas[s][t] = alphas[s][t-1] + matrix[ch][t]\n                elif s == 1:\n                    alphas[s][t] = logsumexp(alphas[s][t-1], alphas[s-1][t-1]) + matrix[ch][t]\n                else:\n                    if ch == blank or ch == mod_sequence[s-2]:\n                        alphas[s][t] = logsumexp(alphas[s][t-1], alphas[s-1][t-1]) + matrix[ch][t]\n                    else:\n                        alphas[s][t] = logsumexp(alphas[s][t-1], alphas[s-1][t-1], alphas[s-2][t-1]) + matrix[ch][t]\n    return alphas","metadata":{"id":"FSU94H8-xJMw","execution":{"iopub.status.busy":"2023-03-19T10:03:34.186111Z","iopub.execute_input":"2023-03-19T10:03:34.186512Z","iopub.status.idle":"2023-03-19T10:03:34.209070Z","shell.execute_reply.started":"2023-03-19T10:03:34.186472Z","shell.execute_reply":"2023-03-19T10:03:34.207965Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## The CTC Backward Algorithm","metadata":{"id":"AOSUt5iox8B6"}},{"cell_type":"code","source":"def backward_algorithm(sequence: List[int], matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    :param sequence: a string converted to an index array by Tokenizer\n    :param matrix: A matrix of shape (K, T) with probability distributions over phonemes at each moment of time.\n    :return: the result of the backward pass of shape (2 * len(sequence) + 1, T)\n    \"\"\"\n    matrix = np.log(matrix)\n    blank = tokenizer.get_symbol_index(BLANK_SYMBOL)\n    mod_sequence = modify_sequence(sequence, blank)\n    betas = np.full([len(mod_sequence), matrix.shape[1]], NEG_INF)\n\n    for t in reversed(range(matrix.shape[1])):\n        for s in reversed(range(len(mod_sequence))):\n            # First Step\n            ch = mod_sequence[s]\n            if t == matrix.shape[1] - 1:\n                if s == betas.shape[0]-1 or s == betas.shape[0]-2:\n                    betas[s][t] = 0\n\n            # Lower Diagonal Zeros\n            elif s > 2 * t + 1:# CONDITION\n                betas[s][t] = NEG_INF\n            else:\n                if s == len(mod_sequence) - 1:\n                    betas[s][t] = betas[s][t+1] + matrix[ch][t]\n                elif s == len(mod_sequence) - 2:\n                    betas[s][t] = logsumexp(betas[s][t+1], betas[s+1][t+1]) + matrix[ch][t]\n                else:\n                    if ch == blank or ch == mod_sequence[s + 2]:\n                            betas[s][t] = logsumexp(betas[s][t+1], betas[s+1][t+1]) + matrix[ch][t]\n                    else:                \n                        betas[s][t] = logsumexp(betas[s][t+1], betas[s+1][t+1], betas[s+2][t+1]) + matrix[ch][t]\n    return betas","metadata":{"id":"hTaA3O8nHArw","execution":{"iopub.status.busy":"2023-03-19T10:03:34.210501Z","iopub.execute_input":"2023-03-19T10:03:34.211359Z","iopub.status.idle":"2023-03-19T10:03:34.226458Z","shell.execute_reply.started":"2023-03-19T10:03:34.211322Z","shell.execute_reply":"2023-03-19T10:03:34.225468Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Soft-Alignment\n","metadata":{"id":"TULzSmkZxgEn"}},{"cell_type":"code","source":"def soft_alignment(labels_indices: List[int], matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns the alignment coefficients for the input sequence\n    \"\"\"\n    alphas = forward_algorithm(labels_indices, matrix)\n    betas = backward_algorithm(labels_indices, matrix)\n\n    # Move from log space back to prob space\n    align = np.exp(alphas + betas)\n\n    # Normalize Alignment\n    align = align / np.sum(align, axis=0)\n\n    return align","metadata":{"id":"r9Xo-1tvHEBX","execution":{"iopub.status.busy":"2023-03-19T10:03:34.230901Z","iopub.execute_input":"2023-03-19T10:03:34.231218Z","iopub.status.idle":"2023-03-19T10:03:34.242637Z","shell.execute_reply.started":"2023-03-19T10:03:34.231190Z","shell.execute_reply":"2023-03-19T10:03:34.241621Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Greedy Best-Path Decoder\n","metadata":{"id":"pQNJqD8mx_0i"}},{"cell_type":"code","source":"#!L\ndef greedy_decoder(output: torch.Tensor, labels: List[torch.Tensor], \n                   label_lengths: List[int], collapse_repeated: bool = True) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    :param output: torch.Tensor of Probs or Log-Probs of shape [batch, time, classes]\n    :param labels: list of label indices converted to torch.Tensors\n    :param label_lengths: list of label lengths (without padding)\n    :param collapse_repeated: whether the repeated characters should be deduplicated\n    :return: the result of the decoding and the target sequence\n    \"\"\"\n    blank_label = tokenizer.get_symbol_index(BLANK_SYMBOL)\n\n    # Get max classes\n    ########################\n    arg_maxes = output.argmax(dim=-1)\n    ########################\n\n    decodes = []\n    targets = []\n\n    # For targets and decodes remove repeats and blanks\n    for i, args in enumerate(arg_maxes):\n        decode = []\n        true_labels = labels[i][:label_lengths[i]].tolist()\n        targets.append(tokenizer.indices_to_text(true_labels))\n\n        # Remove repeats, then remove blanks\n        for j, index in enumerate(args):\n            ########################\n            if j != 0:\n                if index == args[j-1]:\n                    continue\n            decode.append(int(index.cpu().detach()))    \n            ########################\n        ####\n        decode = [x for x in decode if x != blank_label]\n        ######\n        \n        decodes.append(tokenizer.indices_to_text(decode))\n    return decodes, targets","metadata":{"id":"qQhIqk30x4nv","execution":{"iopub.status.busy":"2023-03-19T10:03:34.244072Z","iopub.execute_input":"2023-03-19T10:03:34.244607Z","iopub.status.idle":"2023-03-19T10:03:34.259289Z","shell.execute_reply.started":"2023-03-19T10:03:34.244568Z","shell.execute_reply":"2023-03-19T10:03:34.258180Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## Prefix Decoding With LM","metadata":{"id":"oe_g0pXw21Tn"}},{"cell_type":"code","source":"LanguageModel = TypeVar(\"LanguageModel\")\n# Helper function\n\nclass Beam:\n    def __init__(self, beam_size: int) -> None:\n        self.beam_size = beam_size\n        \n        fn = lambda : (NEG_INF, NEG_INF)\n        \n        # Store probs key - prefix, value - p_blank, p_not_blank for ? step\n        self.candidates = defaultdict(fn)\n        \n        # Store sorted by cumulative probability self.candidates\n        self.top_candidates_list = [\n            (\n                tuple(), \n                (0.0, NEG_INF) # log(p_blank) = 0, log(p_not_blank) = -inf\n            )\n        ]\n        \n    def get_probs_for_prefix(self, prefix: Tuple[int]) -> Tuple[float, float]:\n        p_blank, p_not_blank = self.candidates[prefix]\n        return p_blank, p_not_blank\n        \n    def update_probs_for_prefix(self, prefix: Tuple[int], next_p_blank: float, next_p_not_blank: float) -> None:\n        self.candidates[prefix] = (next_p_blank, next_p_not_blank)\n        \n    def update_top_candidates_list(self) -> None:\n        top_candidates = sorted(\n            self.candidates.items(), \n            key=lambda x: logsumexp(*x[1]), \n            reverse=True\n        )\n        self.top_candidates_list = top_candidates[:self.beam_size]\n        \n\ndef calculate_probability_score_with_lm(lm: LanguageModel, prefix: str) -> float:\n    text = tokenizer.indices_to_text(prefix).upper().strip()    # Use upper case for LM and remove the trailing space\n    lm_prob = lm.log_p(text)             \n    score = lm_prob / np.log10(np.e)    # Convert to natural log, as ARPA LM uses log10   \n    return score","metadata":{"id":"AqYtZjJhY0H0","execution":{"iopub.status.busy":"2023-03-19T10:03:34.260937Z","iopub.execute_input":"2023-03-19T10:03:34.261358Z","iopub.status.idle":"2023-03-19T10:03:34.273751Z","shell.execute_reply.started":"2023-03-19T10:03:34.261318Z","shell.execute_reply":"2023-03-19T10:03:34.272574Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#!L\n\ndef decode(probs: np.ndarray, beam_size: int = 5, lm: Optional[LanguageModel] = None, \n           prune: float = 1e-5, alpha: float = 0.1, beta: float = 2):\n    \"\"\"\n    :param probs: A matrix of shape (T, K) with probability distributions over phonemes at each moment of time.\n    :param beam_size: the size of beams\n    :lm: arpa language model\n    :prune: the minimal probability for a symbol at which it can be added to a prefix\n    :alpha: the parameter to de-weight the LM probability\n    :beta: the parameter to up-weight the length correction term\n    :return: the prefix with the highest sum of probabilites P_blank and P_not_blank\n    \"\"\"\n    T, S = probs.shape\n    probs = np.log(probs)\n    blank = tokenizer.get_symbol_index(BLANK_SYMBOL)\n    space = tokenizer.get_symbol_index(\" \")\n    prune = NEG_INF if prune == 0.0 else np.log(prune)\n    \n    beam = Beam(beam_size)\n    # Итерируемся по оси времени\n    for t in range(T):\n        next_beam = Beam(beam_size)\n        \n        # Итерируемся по символам\n        for s in range(S):\n            p = probs[t, s]\n            # Prune the vocab - пропускаем символ, если вероятность оказаться в нем слишком мала на t-м щаге\n            if p < prune:   \n                continue\n            \n            # Итерируемся по варинатам, в которые можем пойти из текущего символа\n            # Сначала идут наиболее вероятные по сумме log(p_blank + p_not_blank) префиксы\n            # (p_blank, p_not_blank) - вероятности на предыдущем t-1 шаге\n            for prefix, (p_blank, p_not_blank) in beam.top_candidates_list:\n                # Текущий символ - бланк \n                if s == blank:\n                    # вероятности на текущем шаге\n                    p_b, p_nb = next_beam.get_probs_for_prefix(prefix)\n                    next_beam.update_probs_for_prefix(\n                        prefix=prefix,\n                        next_p_blank=logsumexp(p_b, p_blank + p, p_not_blank + p),\n                        next_p_not_blank=p_nb\n                    )\n                    continue\n\n                end_t = prefix[-1] if prefix else None\n                n_prefix = prefix + (s,)\n                \n                # Повторяющийся символ\n                if s == end_t:\n                    # Предыдущий символ - бланк\n                    p_b, p_nb = next_beam.get_probs_for_prefix(n_prefix)\n                    next_beam.update_probs_for_prefix(\n                        prefix=n_prefix,\n                        next_p_blank=p_b,\n                        next_p_not_blank=logsumexp(p_nb, p + p_blank)\n                    )\n                    # Предудщий символ не бланк\n                    p_b, p_nb = next_beam.get_probs_for_prefix(prefix)\n                    next_beam.update_probs_for_prefix(\n                        prefix=prefix,\n                        next_p_blank=p_b,\n                        next_p_not_blank=logsumexp(p_nb, p + p_not_blank)\n                    )\n                elif s == space and end_t is not None and lm is not None:\n                    # Символ - пробел и не первый, нужно применить языковую модель\n                    p_b, p_nb = next_beam.get_probs_for_prefix(n_prefix)\n                    score = calculate_probability_score_with_lm(lm, n_prefix)\n                    length = len(tokenizer.indices_to_text(prefix))\n                    \n                    next_beam.update_probs_for_prefix(\n                        prefix=n_prefix,         \n                        next_p_blank=p_b,\n                        next_p_not_blank=logsumexp(\n                            p_nb,\n                            p_blank + p + score * alpha + np.log(length) * beta,\n                            p_not_blank + p + score * alpha + np.log(length) * beta\n                        )  \n                    )\n                else:\n                    p_b, p_nb = next_beam.get_probs_for_prefix(n_prefix)\n                    next_beam.update_probs_for_prefix(\n                        prefix=n_prefix,\n                        next_p_blank=p_b,\n                        next_p_not_blank=logsumexp(p_nb, p_blank + p, p_not_blank + p)\n                    )\n\n        next_beam.update_top_candidates_list()\n        beam = next_beam\n\n    best = beam.top_candidates_list[0]\n    return best[0], -logsumexp(*best[1])\n\n\ndef beam_search_decoder(probs: np.ndarray, labels: List[List[int]], label_lengths: List[int], \n                        input_lengths: List[int], lm: LanguageModel, beam_size: int = 5,\n                        prune: float = 1e-3, alpha: float = 0.1, beta: float = 0.1):\n    probs = probs.cpu().detach().numpy()\n    decodes, targets = [], []\n    \n    for i, prob in enumerate(probs):\n        targets.append(tokenizer.indices_to_text(labels[i][:label_lengths[i]].tolist()))\n        int_seq, _ = decode(prob[:input_lengths[i]], lm=lm, beam_size=beam_size, prune=prune, alpha=alpha, beta=beta)\n        decodes.append(tokenizer.indices_to_text(int_seq))\n        \n    return decodes, targets","metadata":{"id":"SJ2pts572m5W","execution":{"iopub.status.busy":"2023-03-19T10:03:34.275230Z","iopub.execute_input":"2023-03-19T10:03:34.276350Z","iopub.status.idle":"2023-03-19T10:03:34.296368Z","shell.execute_reply.started":"2023-03-19T10:03:34.276299Z","shell.execute_reply":"2023-03-19T10:03:34.295429Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Homework 2 starts here: CTC Speech Recognition System\nYou can do this notebook in google collab, or use other GPU sources\n\n### Tasks\n\n- (20 points) Train ASR System, WER criterions: 60-50 -- 6 points, 50-40 -- 10 points, 40-35 -- 14 points, <=35 -- 20 points. + 1 bonus point per 1% WER below 30\n- (5 points) Compare performance of DNN, RNN and BiRNN models in terms of WER, training time and other properties\n- (5 points) Compare alignments obtained from DNN, RNN and BiRNN models","metadata":{"id":"doD9f6gZ2RXx"}},{"cell_type":"markdown","source":"## Implementing, training and evaluationg your CTC ASR model","metadata":{"id":"55HlLr7W63JJ"}},{"cell_type":"markdown","source":"### Create a Dataloader\n\nThe first step is to create a dataloader to download and load and preprocess LibriSpeech acoustic data. \n\nThe creative options you have at this stage are:\n\n* The sample rate and number of mel-bins.\n* Various forms of data agumentation","metadata":{"id":"yyU-wpIU67yu"}},{"cell_type":"code","source":"#!L\n# Download LibriSpeech 100hr training and test data\n\nif not os.path.isdir(\"./data\"):\n    os.makedirs(\"./data\")\n\ntrain_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=\"train-clean-100\", download=True)\ntest_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=\"test-clean\", download=True)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["ad2bb120428c441c885cada8939f365b","8a74d3f38d6a4f21baf634ac16c250ff","f2b1c496139943cb94a32ab4bee64861","e68b23df39d44709a7264a1b984202ce","349540f3959540cfa9e37817fe6fb87a","d105d106485d4e6fb293abb0ccfe1d50","2c2c832b70b343ec9d02eb78a49012cd","52a0c4da6c72493081c269b54bd2d34a","187e1adc87184c40aaf7e0e42c9fc12f","5f47531ac07c4c3eb93478200e5da284","3cdc8f0e143a4c11bbc5f221e59afb0c","3699c443d8384126b82546c8375f36c6","0409ea0806ee475398b017fd9c109e79","e7069c91cc5d4d88904f4056df26760c","5d040965274146b8a8af53a4e2d27914","0c777e21e2874f4c98e5ac6761c645d2","cb951ee7c35141d083c45d0a1f3d187e","a59760e5be374f8e803cdfb5062243c9","1cbc864a06144a5f90548c3491d3f3a5","036f72d2c1a2478fa1ecf0856bc309ed","6a4e80d8df644f54954cefbcfbd053b7","34fa8e6ce6f24daabecf30dcf1ec3fd1"]},"id":"qaeFHkHT2jqu","outputId":"e2275185-7223-4f70-cf8e-53c262d526e8","execution":{"iopub.status.busy":"2023-03-19T10:03:34.297765Z","iopub.execute_input":"2023-03-19T10:03:34.298118Z","iopub.status.idle":"2023-03-19T10:08:46.209036Z","shell.execute_reply.started":"2023-03-19T10:03:34.298083Z","shell.execute_reply":"2023-03-19T10:08:46.207987Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/5.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77fc4f95ebc94b4f9c42119893e1231f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/331M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1a96a0fbaaf498c965d3fb6e3e7f809"}},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RandomApply(torch.nn.Module):\n    def __init__(self, transforms, p=0.5):\n        super().__init__()\n        self.transforms = transforms\n        self.p = p\n\n    def forward(self, spec):\n        if self.p < torch.rand(1):\n            return spec\n        for t in self.transforms:\n            spec = t(spec)\n        return spec","metadata":{"execution":{"iopub.status.busy":"2023-03-19T10:08:46.210554Z","iopub.execute_input":"2023-03-19T10:08:46.210921Z","iopub.status.idle":"2023-03-19T10:08:46.219423Z","shell.execute_reply.started":"2023-03-19T10:08:46.210881Z","shell.execute_reply":"2023-03-19T10:08:46.217287Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"class MyAug(torch.nn.Module):\n    def __init__(\n        self,\n        mode = 'test',\n        input_freq=16000, # librispeech freq\n        resample_freq=8000,\n        n_fft=1024,\n        n_mel=256,\n    ):\n        super().__init__()\n        self.mode = mode\n        self.resample = torchaudio.transforms.Resample(orig_freq=input_freq, new_freq=resample_freq)\n\n        self.spec = torchaudio.transforms.Spectrogram(n_fft=n_fft, power=2)\n\n        self.spec_aug = torch.nn.Sequential(\n            RandomApply([torchaudio.transforms.TimeStretch(0.8, fixed_rate=True)], 0.2),\n            RandomApply([torchaudio.transforms.TimeStretch(1.2, fixed_rate=True)], 0.2),\n            RandomApply([torchaudio.transforms.FrequencyMasking(freq_mask_param=50)], 0.4),\n            RandomApply([torchaudio.transforms.TimeMasking(time_mask_param=50)], 0.4)\n        )\n\n        self.mel_scale = torchaudio.transforms.MelScale(\n            n_mels=n_mel, sample_rate=resample_freq, n_stft=n_fft // 2 + 1)\n        self.log = torchaudio.transforms.AmplitudeToDB()\n\n    def forward(self, waveform: torch.Tensor) -> torch.Tensor:\n        # Resample the input\n        # resampled = self.resample(waveform)\n\n        # Convert to power spectrogram\n        spec = self.spec(waveform)\n\n        # Apply SpecAugment\n        if self.mode == 'train':\n            spec = self.spec_aug(spec)\n\n        # Convert to mel-scale\n        mel = self.mel_scale(spec)\n        mel = self.log(mel)\n\n        return mel","metadata":{"execution":{"iopub.status.busy":"2023-03-19T10:08:46.221108Z","iopub.execute_input":"2023-03-19T10:08:46.221815Z","iopub.status.idle":"2023-03-19T10:08:47.344353Z","shell.execute_reply.started":"2023-03-19T10:08:46.221778Z","shell.execute_reply":"2023-03-19T10:08:47.342785Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"#!L\n# For train you can use SpecAugment data aug here.\ntrain_audio_transforms = nn.Sequential(\n    #torchaudio.transforms.Resample(orig_freq=16000, new_freq=8000),\n    torchaudio.transforms.MelSpectrogram(),\n    torchaudio.transforms.AmplitudeToDB(),\n    RandomApply([torchaudio.transforms.TimeStretch(0.8, fixed_rate=True), \n                 torchaudio.transforms.TimeStretch(1.2, fixed_rate=True)], 0.4),\n    RandomApply([torchaudio.transforms.FrequencyMasking(freq_mask_param=80)], 0.5),\n    RandomApply([torchaudio.transforms.TimeMasking(time_mask_param=80)], 0.5)\n)\n\ntest_audio_transforms = nn.Sequential(\n    #torchaudio.transforms.Resample(orig_freq=16000, new_freq=8000),\n    torchaudio.transforms.MelSpectrogram(),\n    torchaudio.transforms.AmplitudeToDB()\n)","metadata":{"execution":{"iopub.status.busy":"2023-03-19T10:20:39.563766Z","iopub.execute_input":"2023-03-19T10:20:39.564420Z","iopub.status.idle":"2023-03-19T10:20:39.592640Z","shell.execute_reply.started":"2023-03-19T10:20:39.564353Z","shell.execute_reply":"2023-03-19T10:20:39.590878Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torchaudio/functional/functional.py:572: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n  \"At least one mel filterbank has all zero values. \"\n","output_type":"stream"}]},{"cell_type":"code","source":"\n#train_audio_transforms = MyAug(mode='train')\n#test_audio_transforms = MyAug(mode='test')","metadata":{"id":"ZFSPjx7zY0H2","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c309b524-1351-41ca-a9bf-48a07b598eab","execution":{"iopub.status.busy":"2023-03-19T10:08:47.837630Z","iopub.execute_input":"2023-03-19T10:08:47.838471Z","iopub.status.idle":"2023-03-19T10:08:47.844360Z","shell.execute_reply.started":"2023-03-19T10:08:47.838432Z","shell.execute_reply":"2023-03-19T10:08:47.843200Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"class Collate:\n    def __init__(self, data_type = 'test') -> None:\n        super(Collate, self).__init__() \n\n        self.data_type = data_type\n\n    def __call__(self, data: torchaudio.datasets.librispeech.LIBRISPEECH) -> Tuple[List[torch.Tensor], ...]:\n        \"\"\"\n        :param data: is a list of tuples of [features, label], where features has dimensions [n_features, length]\n        \"returns features, lengths, labels: \n              features is a Tensor [batchsize, features, max_length]\n              lengths is a Tensor of lengths [batchsize]\n              labels is a Tesnor of targets [batchsize]\n        \"\"\"\n\n        spectrograms = []\n        labels = []\n        input_lengths = []\n        label_lengths = []\n        for (waveform, _, utterance, _, _, _) in data:\n            if self.data_type == 'train':\n                spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n            elif self.data_type == 'test':\n                spec = test_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n            else:\n                raise Exception('data_type should be train or valid')\n            spectrograms.append(spec)\n            label = torch.Tensor(tokenizer.text_to_indices(utterance.lower()))\n            labels.append(label)\n            input_lengths.append(spec.shape[0] // 2)\n            label_lengths.append(len(label))\n\n        spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n        labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n\n        return spectrograms, labels, input_lengths, label_lengths\n","metadata":{"id":"noWmJGQe67IJ","execution":{"iopub.status.busy":"2023-03-19T10:08:47.845882Z","iopub.execute_input":"2023-03-19T10:08:47.847192Z","iopub.status.idle":"2023-03-19T10:08:47.870125Z","shell.execute_reply.started":"2023-03-19T10:08:47.847135Z","shell.execute_reply":"2023-03-19T10:08:47.868662Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"### Implement a Neural Network Model\n\nYou should try out a few different model types:\n- Feed-Forward Model (DNN)\n- Recurrent Model (GRU or LSTM)\n- Bidirectional Recurrent Model (bi-GRU or bi-LSTM)\n- Something different for bonus points\n\nBefore any of this models you can use convolutional layers, as shown in the example below\n\nAfter your experiments you should write a report with comparison of different models in terms of different features, for example: parameters, training speed, resulting quality, spectrogram properties, and data augmentations. Remember, that for full mark you need to achive good WER \n\nWER criterions: 60-50 -- 6 points, 50-40 -- 10 points, 40-35 -- 14 points, <= 35 -- 20 points","metadata":{"id":"kPYITlXT7AoG"}},{"cell_type":"code","source":"class CNNLayerNorm(nn.Module):\n    \"\"\"Layer normalization built for CNNs input\"\"\"\n\n    def __init__(self, n_feats: int) -> None:\n        super(CNNLayerNorm, self).__init__()\n        self.layer_norm = nn.LayerNorm(n_feats)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        # x (batch, channel, feature, time)\n        x = x.transpose(2, 3).contiguous()  # (batch, channel, time, feature)\n        x = self.layer_norm(x)\n        return x.transpose(2, 3).contiguous()  # (batch, channel, feature, time)\n\n\nclass ResidualCNN(nn.Module):\n    \"\"\"Residual CNN inspired by https://arxiv.org/pdf/1603.05027.pdf\n        except with layer norm instead of batch norm\n    \"\"\"\n\n    def __init__(self, in_channels=32, out_channels=32, kernel=3, stride=2, dropout=0.2, n_feats=16) -> None:\n        super(ResidualCNN, self).__init__()\n\n        self.conv_blocks = nn.Sequential(\n            CNNLayerNorm(in_channels*2),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Conv2d(in_channels, n_feats, kernel_size=kernel, padding=1),\n\n            CNNLayerNorm(in_channels*2),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Conv2d(n_feats, out_channels, kernel_size=kernel, padding=1),\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        residual = x  # (batch, channel, feature, time)\n        x = self.conv_blocks(x)\n        x += residual\n        return x  # (batch, channel, feature, time)\n\n\nclass FeatureExtractor(nn.Module):\n\n    def __init__(self, n_cnn_layers: int, n_rnn_layers: int, rnn_dim: int,\n                 n_feats: int, stride: int = 2, dropout: float = 0.1) -> None:\n        super(FeatureExtractor, self).__init__()\n        n_feats = n_feats // 2\n        self.cnn = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=stride, padding=3 // 2)  # cnn for extracting heirachal features\n\n        # n residual cnn layers with filter size of 32\n        self.rescnn_layers = nn.ModuleList([ResidualCNN(32, 32) for i in range(n_cnn_layers)])\n        self.fully_connected = nn.Linear(2048, rnn_dim)\n      \n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.cnn(x)\n        for resblock in self.rescnn_layers:\n            x = resblock(x)\n        sizes = x.size()\n        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n        x = x.transpose(1, 2)  # (batch, time, feature)\n        x = self.fully_connected(x)\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2023-03-19T10:08:47.875713Z","iopub.execute_input":"2023-03-19T10:08:47.879492Z","iopub.status.idle":"2023-03-19T10:08:47.902119Z","shell.execute_reply.started":"2023-03-19T10:08:47.879451Z","shell.execute_reply":"2023-03-19T10:08:47.901088Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"class CTCDNN(nn.Module):\n\n    def __init__(self, n_cnn_layers: int, n_rnn_layers: int, rnn_dim: int, n_class: int, \n                 n_feats: int, stride: int = 2, dropout: float = 0.1) -> None:\n        super(CTCDNN, self).__init__()\n        \n        self.feature_extractor = FeatureExtractor(n_cnn_layers, n_rnn_layers, 64,\n                 n_feats, stride, dropout)\n        \n        layers = []\n        num_layers = 6\n        for i in range(num_layers):\n            if i == 0:\n                dim = 64\n            else:\n                dim = 2048 // 2 ** i\n            layers.append(nn.Linear(dim, 2048 // 2 ** (i +1)))\n            layers.append(nn.GELU())\n            layers.append(nn.Dropout(0.3))\n        self.dnn = nn.Sequential(*layers)\n\n        rnn_dim = 2048 // 2 ** num_layers\n        self.classifier = nn.Linear(rnn_dim, n_class)\n\n    def forward(self, x: torch.Tensor, input_lengths: torch.Tensor) -> torch.Tensor:\n            x = self.feature_extractor(x)\n            for layer in self.dnn:\n                x = layer(x)\n            x = self.classifier(x)\n            return x\n\n\nclass CTCRNN(nn.Module):\n\n    def __init__(self, n_cnn_layers: int, n_rnn_layers: int, rnn_dim: int, n_class: int, \n                 n_feats: int, stride: int = 2, dropout: float = 0.1) -> None:\n        super(CTCRNN, self).__init__()\n\n        self.feature_extractor = FeatureExtractor(n_cnn_layers, n_rnn_layers, rnn_dim,\n                        n_feats, stride, dropout)\n        self.dropout = nn.Dropout(0.2) \n        num_lstm_deep = 6\n        self.lstm = nn.LSTM(2048, rnn_dim, num_lstm_deep)\n        \n        self.preclassifier = nn.Linear(rnn_dim, rnn_dim // 2)\n        self.act = nn.GELU()\n        self.drop_last = nn.Dropout(0.2)\n        self.classifier = nn.Linear(rnn_dim // 2, n_class)\n\n    def forward(self, x: torch.Tensor, input_lengths: torch.Tensor) -> torch.Tensor:\n            x = self.feature_extractor(x)\n            x = self.dropout(x)\n            x, _ = self.lstm(x)\n            \n            x = self.preclassifier(x)\n            x = self.act(x)\n            x = self.drop_last(x)\n            x = self.classifier(x)\n            return x\n\nclass BiGRU(nn.Module):\n    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n        super(BiGRU, self).__init__()\n        self.BiGRU = nn.GRU(\n                input_size=rnn_dim, hidden_size=hidden_size,\n                num_layers=1, batch_first=batch_first, bidirectional=True)\n        self.layer_norm = nn.LayerNorm(rnn_dim)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x):\n        x = self.layer_norm(x)\n        x = F.gelu(x)\n        x, _ = self.BiGRU(x)\n        x = self.dropout(x)\n        return x\n    \nclass BiLSTM(nn.Module):\n    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n        super(BiLSTM, self).__init__()\n        self.BiGRU = nn.LSTM(\n                input_size=rnn_dim, hidden_size=hidden_size,\n                num_layers=1, batch_first=batch_first, bidirectional=True)\n        self.layer_norm = nn.LayerNorm(rnn_dim)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x):\n        x = self.layer_norm(x)\n        x = F.gelu(x)\n        x, _ = self.BiGRU(x)\n        x = self.dropout(x)\n        return x\n    \nclass CTCBiRNN(nn.Module):\n\n    def __init__(self, n_cnn_layers: int, n_rnn_layers: int, rnn_dim: int, n_class: int, \n                 n_feats: int, stride: int = 2, dropout: float = 0.1) -> None:\n        super(CTCBiRNN, self).__init__()\n        \n        self.feature_extractor = FeatureExtractor(n_cnn_layers, n_rnn_layers, rnn_dim,\n                 n_feats, stride, dropout)\n\n        layers = []\n        for i in range(n_rnn_layers):\n            if i == 0:\n                layers.append(BiGRU(rnn_dim=rnn_dim, hidden_size=rnn_dim, dropout=dropout, batch_first=True))\n            else:\n                layers.append(BiGRU(rnn_dim=rnn_dim*2, hidden_size=rnn_dim, dropout=dropout, batch_first=False))\n        self.birnn = nn.Sequential(*layers)\n        \n        self.preclassifier = nn.Linear(rnn_dim * 2, rnn_dim // 2)\n        self.act = nn.GELU()\n        self.drop_last = nn.Dropout(0.2)\n        self.classifier = nn.Linear(rnn_dim // 2, n_class)\n\n    def forward(self, x: torch.Tensor, input_lengths: torch.Tensor) -> torch.Tensor:\n            x = self.feature_extractor(x)\n            x = self.birnn(x)\n            \n            x = self.preclassifier(x)\n            x = self.act(x)\n            x = self.drop_last(x)\n            x = self.classifier(x)\n            return x","metadata":{"id":"c1eUPtQ57EoX","execution":{"iopub.status.busy":"2023-03-19T21:36:34.478546Z","iopub.execute_input":"2023-03-19T21:36:34.480915Z","iopub.status.idle":"2023-03-19T21:36:34.518844Z","shell.execute_reply.started":"2023-03-19T21:36:34.480873Z","shell.execute_reply":"2023-03-19T21:36:34.517627Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"UucOQ7n9LLE1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training and Evaluation Code","metadata":{"id":"fmzKVa497Tav"}},{"cell_type":"code","source":"#!L\n\ndef train(model: nn.Module, device: str, train_loader: data.DataLoader, \n          criterion: nn.Module, optimizer: torch.optim.Optimizer, \n          scheduler: torch.optim.lr_scheduler, epoch: int) -> None:\n    model.train()\n    data_len = len(train_loader.dataset)\n    for batch_idx, _data in enumerate(train_loader):\n        spectrograms, labels, input_lengths, label_lengths = _data\n        spectrograms, labels = spectrograms.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n\n        output = model(spectrograms, input_lengths)  # (batch, time, n_class)\n        output = F.log_softmax(output, dim=2)\n        output = output.transpose(0, 1)  # (time, batch, n_class)\n\n        loss = criterion(output, labels, input_lengths, label_lengths)\n        loss.backward()\n\n        optimizer.step()\n        scheduler.step()\n        if batch_idx % 100 == 0 or batch_idx == data_len:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(spectrograms), data_len,\n                       100. * batch_idx / len(train_loader), loss.item()))\n            wandb.log({'loss_train': loss.item()})\n        \n\n\ndef test(model: nn.Module, device: str, test_loader: data.DataLoader, \n         criterion: nn.Module, epoch: int, decode: str = 'Greedy', lm: LanguageModel = None, save_path: str = None) -> None:\n    print('Beginning eval...')\n    model.eval()\n    test_loss = 0\n    test_cer, test_wer = [], []\n    with torch.no_grad():\n        start = time.time()\n        for i, _data in enumerate(test_loader):\n            spectrograms, labels, input_lengths, label_lengths = _data\n            spectrograms, labels = spectrograms.to(device), labels.to(device)\n            \n            matrix = model(spectrograms, input_lengths)  # (batch, time, n_class)\n            matrix = F.log_softmax(matrix, dim=2)\n            probs = F.softmax(matrix,dim=2)\n            matrix = matrix.transpose(0, 1)  # (time, batch, n_class)\n                \n            if i == 3:\n                np.savetxt(f\"{save_path}_matrix.txt\", probs[0].cpu().numpy())\n                np.savetxt(f\"{save_path}_labels.txt\", labels[0].cpu().numpy())\n\n            loss = criterion(matrix, labels, input_lengths, label_lengths)\n            test_loss += loss.item() / len(test_loader)\n\n            if decode == 'Greedy':\n                decoded_preds, decoded_targets = greedy_decoder(matrix.transpose(0, 1), labels, label_lengths)\n            elif decode == 'BeamSearch':\n                ## THIS IS THE FUNCTION YOU SHOULD IMPLEMENT\n                decoded_preds, decoded_targets = beam_search_decoder(probs, labels, label_lengths, input_lengths, lm=lm)\n            for j in range(len(decoded_preds)):\n                test_cer.append(utils.cer(decoded_targets[j], decoded_preds[j]))\n                test_wer.append(utils.wer(decoded_targets[j], decoded_preds[j]))\n\n    avg_cer = sum(test_cer) / len(test_cer)\n    avg_wer = sum(test_wer) / len(test_wer)\n    wandb.log({'loss_test': test_loss, 'avg_cer': avg_cer, 'avg_wer': avg_wer})\n    print(\n        'Epoch: {:d}, Test set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'.format(epoch, test_loss,\n                                                                                                       avg_cer,\n                                                                                                       avg_wer))","metadata":{"id":"v_O37QIX3nft","execution":{"iopub.status.busy":"2023-03-19T10:08:47.960719Z","iopub.execute_input":"2023-03-19T10:08:47.963588Z","iopub.status.idle":"2023-03-19T10:08:47.987856Z","shell.execute_reply.started":"2023-03-19T10:08:47.963549Z","shell.execute_reply":"2023-03-19T10:08:47.986694Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"torch.manual_seed(7)\nif torch.cuda.is_available():\n    print('GPU found! 🎉')\n    device = 'cuda'\nelse:\n    print('Only CPU found! 💻')\n    device = 'cpu'\n\nverbose=False\n\n# Hyperparameters for your model\nhparams = {\n    \"n_cnn_layers\": 5,\n    \"n_rnn_layers\": 3,\n    \"rnn_dim\": 512,\n    \"n_class\": 29,\n    \"n_feats\": 64,\n    \"stride\": 2,\n    \"dropout\": 0.3,\n    \"learning_rate\":  3e-4,\n    \"batch_size\": 32,\n    \"epochs\": 40\n}\n\ntrain_collate_fn = Collate(data_type='train')\ntest_collate_fn = Collate(data_type='test')\n\n# Define Dataloyour training and test data loaders\nkwargs = {'num_workers': 2, 'pin_memory': True} if device == 'cuda' else {}\ntrain_loader = data.DataLoader(train_dataset, batch_size=hparams['batch_size'], shuffle=True, collate_fn=train_collate_fn, **kwargs)\n\nkwargs = {'num_workers': 1, 'pin_memory': True} if device == 'cuda' else {}\ntest_loader = data.DataLoader(test_dataset, batch_size=hparams['batch_size'], shuffle=False, collate_fn=test_collate_fn, **kwargs)","metadata":{"id":"iO-mKLPt7Xhb","outputId":"05acdb4d-bc93-4f36-9c8a-18761e920d88","colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2023-03-19T21:20:52.334732Z","iopub.execute_input":"2023-03-19T21:20:52.335189Z","iopub.status.idle":"2023-03-19T21:20:52.356232Z","shell.execute_reply.started":"2023-03-19T21:20:52.335142Z","shell.execute_reply":"2023-03-19T21:20:52.354541Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"GPU found! 🎉\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"id":"8TwDM_rafafp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We recommend to use \"Weights & Biases\" for experiment logging. See their [documentation](https://docs.wandb.ai/) for detais.","metadata":{"id":"q_uhuVgw2XbB","outputId":"5910f7ac-260c-4506-b910-500bc8dcd645"}},{"cell_type":"code","source":"wandb.init(project=\"hw2-dlaudio\", \n           group=\"BiRNN_gru\",\n           config=hparams) # 7d600dfaf11a4c3bc2b44362da8ac75b3e59f691","metadata":{"id":"3cVNk7KkY0H4","outputId":"94d32574-d6db-419f-8351-d2625343300d","colab":{"base_uri":"https://localhost:8080/","height":161},"execution":{"iopub.status.busy":"2023-03-19T21:35:57.513271Z","iopub.execute_input":"2023-03-19T21:35:57.514449Z","iopub.status.idle":"2023-03-19T21:36:34.473449Z","shell.execute_reply.started":"2023-03-19T21:35:57.514392Z","shell.execute_reply":"2023-03-19T21:36:34.472447Z"},"trusted":true},"execution_count":37,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Finishing last run (ID:f3eqiqcg) before initializing another..."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_cer</td><td>▂▂▂▂▃▂▂▂▂█▂▂▁▁▁▁▁▃</td></tr><tr><td>avg_wer</td><td>▃▃▃▃▃▃▃▂▂█▂▂▂▁▁▁▁▃</td></tr><tr><td>loss_test</td><td>▂▂▂▂▂▂▂▂▂█▂▂▁▁▁▁▁▃</td></tr><tr><td>loss_train</td><td>▁▂▂▂▁▂▂▂▂▂▁▂▂▂▂▂▁▂▁▁▂▂▂▁▁▁▂▁▂▁▁▁▁▁▂▂▂▂▂█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_cer</td><td>0.19951</td></tr><tr><td>avg_wer</td><td>0.57189</td></tr><tr><td>loss_test</td><td>0.78823</td></tr><tr><td>loss_train</td><td>2.87864</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">gentle-shape-34</strong> at: <a href='https://wandb.ai/eksolodneva/hw2-dlaudio/runs/f3eqiqcg' target=\"_blank\">https://wandb.ai/eksolodneva/hw2-dlaudio/runs/f3eqiqcg</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 40 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20230319_101552-f3eqiqcg/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Successfully finished last run (ID:f3eqiqcg). Initializing new run:<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.14.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.13.10"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20230319_213557-3s567ggd</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/eksolodneva/hw2-dlaudio/runs/3s567ggd' target=\"_blank\">still-voice-35</a></strong> to <a href='https://wandb.ai/eksolodneva/hw2-dlaudio' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/eksolodneva/hw2-dlaudio' target=\"_blank\">https://wandb.ai/eksolodneva/hw2-dlaudio</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/eksolodneva/hw2-dlaudio/runs/3s567ggd' target=\"_blank\">https://wandb.ai/eksolodneva/hw2-dlaudio/runs/3s567ggd</a>"},"metadata":{}},{"execution_count":37,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/eksolodneva/hw2-dlaudio/runs/3s567ggd?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7fde813b9c90>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Compare different models: DNN, GRU/LSTM, bi-GRU/bi-LSTM (5 points)\n\nTrain and discuss differences in the different models. \n\nCompare performance of DNN, RNN and BiRNN models in terms of:\n-  WER / CER \n-  Training time\n-  Training stability \n-  Any other properties?","metadata":{"id":"gB79UDNM8UOZ"}},{"cell_type":"code","source":"# Train a non-recurrent model\nctc_dnn = CTCDNN(\n    hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n    hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout'])\n\nif torch.cuda.device_count() > 1:\n    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n    ctc_dnn = nn.DataParallel(ctc_dnn)\n\nctc_dnn.to(device)\n\n\noptimizer = torch.optim.Adam(ctc_dnn.parameters(), lr=hparams['learning_rate'])\ncriterion = nn.CTCLoss(blank=tokenizer.get_symbol_index(BLANK_SYMBOL), reduction='mean')\nscheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, hparams['learning_rate'], epochs=hparams['epochs'], steps_per_epoch=len(train_loader))\n\nfor epoch in tqdm(range(1, hparams['epochs'] + 1)):\n    train(ctc_dnn, device, train_loader, criterion, optimizer, scheduler, epoch)\n    utils.save_checkpoint(ctc_dnn, checkpoint_name=f'ctc_dnn_epoch{epoch}.tar')\n    wandb.save(f'ctc_dnn_epoch{epoch}.tar')\n    test(ctc_dnn, device, test_loader, criterion, epoch,decode='BeamSearch', lm='dnn')\n\nutils.save_checkpoint(ctc_dnn, checkpoint_name=f'ctc_dnn.tar')","metadata":{"id":"YJZH5l2YY0H5","colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["f03ca9c3cde54cc2ac450576dce0ab91","917c29ccb5bd4e8180c1cf6389687dea","f891d5b9693249e5b268e2b5fc400f27","29897923372345dfbc32db2bdffa5685","cf49764871814a1796d804c114bb9fa7","b077546f6441468fa8a9b3f8eb441175","7a82b7eec1d44128ac7f623dc246f4ec","814c65ce3221412080e16085af0f4db5","99d097ca76b84f2d8de69490144a2e87","cbe0e86841b8465dbc0db2a2a2546a43","7ca4f0a7009842489b23f4a20f5b6424"]},"outputId":"c3b1e695-4c7b-4036-f530-9dc6d235e31c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train a  recurrent model\nctc_rnn = CTCRNN(\n    hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n    hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout']\n).to(device)\n\nif torch.cuda.device_count() > 1:\n    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n    ctc_rnn = nn.DataParallel(ctc_rnn)\n\nctc_rnn.to(device)\n\noptimizer = torch.optim.Adam(ctc_rnn.parameters(), lr=hparams['learning_rate'])\ncriterion = nn.CTCLoss(blank=tokenizer.get_symbol_index(BLANK_SYMBOL), reduction='mean')\nscheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, hparams['learning_rate'], epochs=hparams['epochs'], steps_per_epoch=len(train_loader))\n\nfor epoch in tqdm(range(1, hparams['epochs'] + 1)):\n    train(ctc_rnn, device, train_loader, criterion, optimizer, scheduler, epoch)\n    utils.save_checkpoint(ctc_rnn, checkpoint_name=f'ctc_rnn_epoch{epoch}.tar')\n    wandb.save(f'ctc_rnn_epoch{epoch}.tar')\n    test(ctc_rnn, device, test_loader, criterion, epoch, decode='BeamSearch', lm='rnn')\n\nutils.save_checkpoint(ctc_rnn, checkpoint_name=f'ctc_rnn.tar')","metadata":{"id":"EnjLgyVrWq83","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["8d1046874acf4d2e923ef8cccc996230","054174569ed84ac2b1d50b29c3cb233d","59e3cccf8d4a4e91ae5c3179c53ecf4e","fc3d39bf7b25494699cee8938619c63f","6b96af8dcca3409a8435cf699ea1c101","5c40a1c55d934537b197b632b2237f5e","d6c3ba653c1a487bb4a9ba5120a87b68","ea4a8e1fbd2449e59b48eba4cccab134","b5cbdbb9c60a4d40974a2c9e4254d06c","23292d20b0274432be17e73de63692b0","6c734cea188c47dd88787f917bc29c5a"]},"outputId":"2baa26c3-7c90-4d04-f05c-72b15c84712c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ctc_birnn = CTCBiRNN(\n    hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n    hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout']\n).to(device)\n\n\n#utils.load_checkpoint(ctc_birnn.module,checkpoint_name='ctc_birnn_epoch10.tar',path='/kaggle/input/rnn-lstm-1', device=device)\nif torch.cuda.device_count() > 1:\n    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n    ctc_birnn = nn.DataParallel(ctc_birnn)\n\n#state_d = {key[7:]: item for key, item in torch.load('/kaggle/working/ctc_birnn_epoch12.tar')['model_state_dict'].items()}\n#ctc_birnn.module.load_state_dict(state_d)","metadata":{"execution":{"iopub.status.busy":"2023-03-19T21:36:34.524690Z","iopub.execute_input":"2023-03-19T21:36:34.527440Z","iopub.status.idle":"2023-03-19T21:36:34.764127Z","shell.execute_reply.started":"2023-03-19T21:36:34.527397Z","shell.execute_reply":"2023-03-19T21:36:34.763126Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"Let's use 2 GPUs!\n","output_type":"stream"}]},{"cell_type":"code","source":"# Train a  recurrent model\n\n\n\n#ctc_birnn.to(device)\n\noptimizer = torch.optim.Adam(ctc_birnn.parameters(), lr=hparams['learning_rate'])\n# optimizer = torch.optim.SGD(ctc_birnn.parameters(), lr=3e-5, momentum=0.9)\ncriterion = nn.CTCLoss(blank=tokenizer.get_symbol_index(BLANK_SYMBOL), reduction='mean')\nscheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, 1e-3, epochs=hparams['epochs'], steps_per_epoch=len(train_loader))\n\nfor epoch in tqdm(range(1, hparams['epochs'] + 1)):\n    train(ctc_birnn, device, train_loader, criterion, optimizer, scheduler, epoch)\n    utils.save_checkpoint(ctc_birnn, checkpoint_name=f'ctc_birnn_epoch{epoch}.tar')\n    wandb.save(f'ctc_birnn_epoch{epoch}.tar')\n    if epoch % 5 == 0:\n        test(ctc_birnn, device, test_loader, criterion, epoch, decode='BeamSearch')\n\nutils.save_checkpoint(ctc_birnn, checkpoint_name=f'ctc_birnn.tar')","metadata":{"id":"B4ylbT6eWqx_","execution":{"iopub.status.busy":"2023-03-19T21:37:20.824618Z","iopub.execute_input":"2023-03-19T21:37:20.825011Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/40 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95c7198b5930465db4db115fd4c3d94c"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/rnn.py:956: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/cudnn/RNN.cpp:968.)\n  self.dropout, self.training, self.bidirectional, self.batch_first)\n","output_type":"stream"},{"name":"stdout","text":"Train Epoch: 1 [0/28539 (0%)]\tLoss: 7.069934\nTrain Epoch: 1 [3200/28539 (11%)]\tLoss: 2.902110\nTrain Epoch: 1 [6400/28539 (22%)]\tLoss: 2.874933\nTrain Epoch: 1 [9600/28539 (34%)]\tLoss: 2.874090\nTrain Epoch: 1 [12800/28539 (45%)]\tLoss: 2.885397\nTrain Epoch: 1 [16000/28539 (56%)]\tLoss: 2.871876\nTrain Epoch: 1 [19200/28539 (67%)]\tLoss: 2.869170\nTrain Epoch: 1 [22400/28539 (78%)]\tLoss: 2.881382\nTrain Epoch: 1 [25600/28539 (90%)]\tLoss: 2.868397\nTrain Epoch: 2 [0/28539 (0%)]\tLoss: 2.855856\nTrain Epoch: 2 [3200/28539 (11%)]\tLoss: 2.865240\nTrain Epoch: 2 [6400/28539 (22%)]\tLoss: 2.856064\nTrain Epoch: 2 [9600/28539 (34%)]\tLoss: 2.887931\nTrain Epoch: 2 [12800/28539 (45%)]\tLoss: 2.857551\nTrain Epoch: 2 [16000/28539 (56%)]\tLoss: 2.824036\nTrain Epoch: 2 [19200/28539 (67%)]\tLoss: 2.641940\nTrain Epoch: 2 [22400/28539 (78%)]\tLoss: 2.341880\nTrain Epoch: 2 [25600/28539 (90%)]\tLoss: 2.140161\nTrain Epoch: 3 [0/28539 (0%)]\tLoss: 2.109456\nTrain Epoch: 3 [3200/28539 (11%)]\tLoss: 1.956791\nTrain Epoch: 3 [6400/28539 (22%)]\tLoss: 1.940060\nTrain Epoch: 3 [9600/28539 (34%)]\tLoss: 1.859704\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Compare alignments (5 points)\n\nIn this section you should compare alignments obtained from different models (DNN / RNN / BiRNN). For example, you can show:\n\n- Examples of alignments and their analysis. \n- Differencies in the properties of alignment distributions over the dataset. \n- Dynamic of alignments during training (from checkpoints). \n- Connection between alignments and model loss. \n- Which models use the most blanks and why?","metadata":{"id":"aVLB1aH_ljcw"}},{"cell_type":"code","source":"# Some code to get you started.\ndnn_matrix = np.loadtxt('dnn_matrix.txt')\nrnn_matrix = np.loadtxt('rnn_matrix.txt')\nbirnn_matrix = np.loadtxt('birnn_matrix.txt')\n\ndnn_labels = np.loadtxt('dnn_labels.txt', dtype=np.int32)\nrnn_labels = np.loadtxt('rnn_labels.txt', dtype=np.int32)\nbirnn_labels = np.loadtxt('birnn_labels.txt', dtype=np.int32)\n\ndnn_align = soft_alignment(dnn_labels, dnn_matrix)\nrnn_align = soft_alignment(rnn_labels, rnn_matrix)\nbirnn_align = soft_alignment(birnn_labels, birnn_matrix)\n\nf, ax = plt.subplots(3, 2, dpi=75, figsize=(15, 15))\n\n\nim = ax[0,0].imshow(dnn_align, aspect='auto', interpolation='nearest')\nax[0,0].set_title(\"DNN Alignment\")\nax[0,0].set_ylabel(\"Phonemes\")\nax[0,0].set_xlabel(\"Time\")\nf.colorbar(im, ax=ax[0,0])\n\nim = ax[0,1].imshow(np.log(dnn_align), aspect='auto', interpolation='nearest')\nax[0,1].set_title(\"DNN Alignment in log scale\")\nax[0,1].set_ylabel(\"Phonemes\")\nax[0,1].set_xlabel(\"Time\")\nf.colorbar(im, ax=ax[0,1])\n\nim = ax[1,0].imshow(dnn_align, aspect='auto', interpolation='nearest')\nax[1,0].set_title(\"RNN Alignment\")\nax[1,0].set_ylabel(\"Phonemes\")\nax[1,0].set_xlabel(\"Time\")\nf.colorbar(im, ax=ax[1,0])\n\nim = ax[1,1].imshow(np.log(dnn_align), aspect='auto', interpolation='nearest')\nax[1,1].set_title(\"RNN Alignment in log scale\")\nax[1,1].set_ylabel(\"Phonemes\")\nax[1,1].set_xlabel(\"Time\")\nf.colorbar(im, ax=ax[1,1])\n\nim = ax[2,0].imshow(dnn_align, aspect='auto', interpolation='nearest')\nax[2,0].set_title(\"BiRNN Alignment\")\nax[2,0].set_ylabel(\"Phonemes\")\nax[2,0].set_xlabel(\"Time\")\nf.colorbar(im, ax=ax[2,0])\n\nim = ax[2,1].imshow(np.log(dnn_align), aspect='auto', interpolation='nearest')\nax[2,1].set_title(\"BiRNN Alignment in log scale\")\nax[2,1].set_ylabel(\"Phonemes\")\nax[2,1].set_xlabel(\"Time\")\nf.colorbar(im, ax=ax[2,1])\n\nplt.tight_layout()","metadata":{"id":"lF0rnu1O3kdU","trusted":true},"execution_count":null,"outputs":[]}]}
